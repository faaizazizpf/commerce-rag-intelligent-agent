{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.22638092363416842,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001509206157561123,
      "grad_norm": 4.526779651641846,
      "learning_rate": 0.0004983333333333334,
      "loss": 1.7812,
      "step": 10
    },
    {
      "epoch": 0.003018412315122246,
      "grad_norm": 3.6299264430999756,
      "learning_rate": 0.0004966666666666666,
      "loss": 1.2615,
      "step": 20
    },
    {
      "epoch": 0.004527618472683369,
      "grad_norm": 2.523728370666504,
      "learning_rate": 0.000495,
      "loss": 0.8381,
      "step": 30
    },
    {
      "epoch": 0.006036824630244492,
      "grad_norm": 4.617517948150635,
      "learning_rate": 0.0004933333333333334,
      "loss": 1.1124,
      "step": 40
    },
    {
      "epoch": 0.007546030787805615,
      "grad_norm": 4.280418872833252,
      "learning_rate": 0.0004916666666666666,
      "loss": 1.0023,
      "step": 50
    },
    {
      "epoch": 0.009055236945366738,
      "grad_norm": 4.2745842933654785,
      "learning_rate": 0.00049,
      "loss": 1.1932,
      "step": 60
    },
    {
      "epoch": 0.01056444310292786,
      "grad_norm": 3.2863259315490723,
      "learning_rate": 0.0004883333333333333,
      "loss": 1.0693,
      "step": 70
    },
    {
      "epoch": 0.012073649260488983,
      "grad_norm": 4.149944305419922,
      "learning_rate": 0.0004866666666666667,
      "loss": 1.0359,
      "step": 80
    },
    {
      "epoch": 0.013582855418050106,
      "grad_norm": 5.123433589935303,
      "learning_rate": 0.00048499999999999997,
      "loss": 1.15,
      "step": 90
    },
    {
      "epoch": 0.01509206157561123,
      "grad_norm": 2.4548261165618896,
      "learning_rate": 0.00048333333333333334,
      "loss": 0.9322,
      "step": 100
    },
    {
      "epoch": 0.016601267733172352,
      "grad_norm": 2.6163525581359863,
      "learning_rate": 0.0004816666666666667,
      "loss": 1.052,
      "step": 110
    },
    {
      "epoch": 0.018110473890733475,
      "grad_norm": 3.650675058364868,
      "learning_rate": 0.00048,
      "loss": 1.4148,
      "step": 120
    },
    {
      "epoch": 0.019619680048294598,
      "grad_norm": 4.163342475891113,
      "learning_rate": 0.0004783333333333333,
      "loss": 1.107,
      "step": 130
    },
    {
      "epoch": 0.02112888620585572,
      "grad_norm": 2.6465706825256348,
      "learning_rate": 0.0004766666666666667,
      "loss": 0.9945,
      "step": 140
    },
    {
      "epoch": 0.022638092363416844,
      "grad_norm": 4.102530002593994,
      "learning_rate": 0.000475,
      "loss": 0.784,
      "step": 150
    },
    {
      "epoch": 0.024147298520977967,
      "grad_norm": 5.171638011932373,
      "learning_rate": 0.00047333333333333336,
      "loss": 1.2555,
      "step": 160
    },
    {
      "epoch": 0.02565650467853909,
      "grad_norm": 4.003929615020752,
      "learning_rate": 0.0004716666666666667,
      "loss": 0.9605,
      "step": 170
    },
    {
      "epoch": 0.027165710836100213,
      "grad_norm": 4.162509918212891,
      "learning_rate": 0.00047,
      "loss": 1.0895,
      "step": 180
    },
    {
      "epoch": 0.028674916993661335,
      "grad_norm": 5.338866233825684,
      "learning_rate": 0.00046833333333333335,
      "loss": 1.152,
      "step": 190
    },
    {
      "epoch": 0.03018412315122246,
      "grad_norm": 3.918215274810791,
      "learning_rate": 0.00046666666666666666,
      "loss": 1.0471,
      "step": 200
    },
    {
      "epoch": 0.03169332930878358,
      "grad_norm": 2.553309440612793,
      "learning_rate": 0.000465,
      "loss": 1.2008,
      "step": 210
    },
    {
      "epoch": 0.033202535466344704,
      "grad_norm": 5.488369464874268,
      "learning_rate": 0.00046333333333333334,
      "loss": 1.3684,
      "step": 220
    },
    {
      "epoch": 0.03471174162390583,
      "grad_norm": 3.3953678607940674,
      "learning_rate": 0.0004616666666666667,
      "loss": 1.0859,
      "step": 230
    },
    {
      "epoch": 0.03622094778146695,
      "grad_norm": 5.445623874664307,
      "learning_rate": 0.00046,
      "loss": 0.8508,
      "step": 240
    },
    {
      "epoch": 0.03773015393902807,
      "grad_norm": 2.3504083156585693,
      "learning_rate": 0.0004583333333333333,
      "loss": 1.0516,
      "step": 250
    },
    {
      "epoch": 0.039239360096589196,
      "grad_norm": 8.683120727539062,
      "learning_rate": 0.0004566666666666667,
      "loss": 1.1355,
      "step": 260
    },
    {
      "epoch": 0.04074856625415032,
      "grad_norm": 6.5666961669921875,
      "learning_rate": 0.000455,
      "loss": 0.9324,
      "step": 270
    },
    {
      "epoch": 0.04225777241171144,
      "grad_norm": 8.30313491821289,
      "learning_rate": 0.0004533333333333333,
      "loss": 1.1682,
      "step": 280
    },
    {
      "epoch": 0.043766978569272565,
      "grad_norm": 2.2594666481018066,
      "learning_rate": 0.0004516666666666667,
      "loss": 1.2047,
      "step": 290
    },
    {
      "epoch": 0.04527618472683369,
      "grad_norm": 4.166910171508789,
      "learning_rate": 0.00045000000000000004,
      "loss": 1.1027,
      "step": 300
    },
    {
      "epoch": 0.04678539088439481,
      "grad_norm": 3.9617397785186768,
      "learning_rate": 0.0004483333333333333,
      "loss": 1.0277,
      "step": 310
    },
    {
      "epoch": 0.04829459704195593,
      "grad_norm": 3.49330997467041,
      "learning_rate": 0.00044666666666666666,
      "loss": 1.0688,
      "step": 320
    },
    {
      "epoch": 0.049803803199517056,
      "grad_norm": 2.9284489154815674,
      "learning_rate": 0.00044500000000000003,
      "loss": 1.0012,
      "step": 330
    },
    {
      "epoch": 0.05131300935707818,
      "grad_norm": 2.4470107555389404,
      "learning_rate": 0.00044333333333333334,
      "loss": 1.0434,
      "step": 340
    },
    {
      "epoch": 0.0528222155146393,
      "grad_norm": 5.3538641929626465,
      "learning_rate": 0.00044166666666666665,
      "loss": 0.9137,
      "step": 350
    },
    {
      "epoch": 0.054331421672200425,
      "grad_norm": 4.424758434295654,
      "learning_rate": 0.00044,
      "loss": 1.0152,
      "step": 360
    },
    {
      "epoch": 0.05584062782976155,
      "grad_norm": 2.2221152782440186,
      "learning_rate": 0.0004383333333333334,
      "loss": 1.0074,
      "step": 370
    },
    {
      "epoch": 0.05734983398732267,
      "grad_norm": 8.342182159423828,
      "learning_rate": 0.00043666666666666664,
      "loss": 0.8652,
      "step": 380
    },
    {
      "epoch": 0.058859040144883794,
      "grad_norm": 4.247380256652832,
      "learning_rate": 0.000435,
      "loss": 0.9426,
      "step": 390
    },
    {
      "epoch": 0.06036824630244492,
      "grad_norm": 3.9363439083099365,
      "learning_rate": 0.00043333333333333337,
      "loss": 0.7859,
      "step": 400
    },
    {
      "epoch": 0.06187745246000604,
      "grad_norm": 3.390939950942993,
      "learning_rate": 0.0004316666666666667,
      "loss": 0.9373,
      "step": 410
    },
    {
      "epoch": 0.06338665861756716,
      "grad_norm": 1.9428819417953491,
      "learning_rate": 0.00043,
      "loss": 1.0242,
      "step": 420
    },
    {
      "epoch": 0.06489586477512828,
      "grad_norm": 2.896209955215454,
      "learning_rate": 0.00042833333333333335,
      "loss": 0.8498,
      "step": 430
    },
    {
      "epoch": 0.06640507093268941,
      "grad_norm": 5.313353538513184,
      "learning_rate": 0.0004266666666666667,
      "loss": 0.9557,
      "step": 440
    },
    {
      "epoch": 0.06791427709025052,
      "grad_norm": 4.082955360412598,
      "learning_rate": 0.000425,
      "loss": 0.9406,
      "step": 450
    },
    {
      "epoch": 0.06942348324781165,
      "grad_norm": 4.587948322296143,
      "learning_rate": 0.00042333333333333334,
      "loss": 0.8439,
      "step": 460
    },
    {
      "epoch": 0.07093268940537277,
      "grad_norm": 4.431200981140137,
      "learning_rate": 0.0004216666666666667,
      "loss": 1.1678,
      "step": 470
    },
    {
      "epoch": 0.0724418955629339,
      "grad_norm": 3.31732439994812,
      "learning_rate": 0.00042,
      "loss": 0.8273,
      "step": 480
    },
    {
      "epoch": 0.07395110172049502,
      "grad_norm": 1.845638632774353,
      "learning_rate": 0.00041833333333333333,
      "loss": 0.8957,
      "step": 490
    },
    {
      "epoch": 0.07546030787805615,
      "grad_norm": 1.3633835315704346,
      "learning_rate": 0.0004166666666666667,
      "loss": 0.9088,
      "step": 500
    },
    {
      "epoch": 0.07546030787805615,
      "eval_bleu-4": 0.046722778375846534,
      "eval_rouge-1": 27.196036875439834,
      "eval_rouge-2": 25.658205770584097,
      "eval_rouge-l": 8.26850239268121,
      "eval_runtime": 502.6535,
      "eval_samples_per_second": 2.827,
      "eval_steps_per_second": 0.708,
      "step": 500
    },
    {
      "epoch": 0.07696951403561726,
      "grad_norm": 3.836848020553589,
      "learning_rate": 0.000415,
      "loss": 1.2309,
      "step": 510
    },
    {
      "epoch": 0.07847872019317839,
      "grad_norm": 1.6586923599243164,
      "learning_rate": 0.0004133333333333333,
      "loss": 0.8529,
      "step": 520
    },
    {
      "epoch": 0.07998792635073951,
      "grad_norm": 2.31618332862854,
      "learning_rate": 0.0004116666666666667,
      "loss": 0.9672,
      "step": 530
    },
    {
      "epoch": 0.08149713250830064,
      "grad_norm": 5.9583330154418945,
      "learning_rate": 0.00041,
      "loss": 1.1178,
      "step": 540
    },
    {
      "epoch": 0.08300633866586175,
      "grad_norm": 3.986414670944214,
      "learning_rate": 0.00040833333333333336,
      "loss": 0.8846,
      "step": 550
    },
    {
      "epoch": 0.08451554482342288,
      "grad_norm": 9.42641544342041,
      "learning_rate": 0.00040666666666666667,
      "loss": 1.0516,
      "step": 560
    },
    {
      "epoch": 0.086024750980984,
      "grad_norm": 4.890380382537842,
      "learning_rate": 0.00040500000000000003,
      "loss": 1.098,
      "step": 570
    },
    {
      "epoch": 0.08753395713854513,
      "grad_norm": 6.560605525970459,
      "learning_rate": 0.00040333333333333334,
      "loss": 0.9654,
      "step": 580
    },
    {
      "epoch": 0.08904316329610625,
      "grad_norm": 3.294018030166626,
      "learning_rate": 0.00040166666666666665,
      "loss": 0.9832,
      "step": 590
    },
    {
      "epoch": 0.09055236945366738,
      "grad_norm": 2.524383306503296,
      "learning_rate": 0.0004,
      "loss": 0.9836,
      "step": 600
    },
    {
      "epoch": 0.09206157561122849,
      "grad_norm": 3.1457579135894775,
      "learning_rate": 0.00039833333333333333,
      "loss": 1.2008,
      "step": 610
    },
    {
      "epoch": 0.09357078176878962,
      "grad_norm": 4.523575782775879,
      "learning_rate": 0.0003966666666666667,
      "loss": 1.0,
      "step": 620
    },
    {
      "epoch": 0.09507998792635074,
      "grad_norm": 3.7047181129455566,
      "learning_rate": 0.000395,
      "loss": 0.9293,
      "step": 630
    },
    {
      "epoch": 0.09658919408391187,
      "grad_norm": 1.5108058452606201,
      "learning_rate": 0.0003933333333333333,
      "loss": 0.8359,
      "step": 640
    },
    {
      "epoch": 0.09809840024147298,
      "grad_norm": 3.422895908355713,
      "learning_rate": 0.0003916666666666667,
      "loss": 0.9664,
      "step": 650
    },
    {
      "epoch": 0.09960760639903411,
      "grad_norm": 3.7136294841766357,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.8592,
      "step": 660
    },
    {
      "epoch": 0.10111681255659523,
      "grad_norm": 2.342552423477173,
      "learning_rate": 0.0003883333333333333,
      "loss": 0.891,
      "step": 670
    },
    {
      "epoch": 0.10262601871415636,
      "grad_norm": 3.4248383045196533,
      "learning_rate": 0.00038666666666666667,
      "loss": 0.8959,
      "step": 680
    },
    {
      "epoch": 0.10413522487171747,
      "grad_norm": 2.955563545227051,
      "learning_rate": 0.00038500000000000003,
      "loss": 1.0521,
      "step": 690
    },
    {
      "epoch": 0.1056444310292786,
      "grad_norm": 1.354830026626587,
      "learning_rate": 0.00038333333333333334,
      "loss": 0.9434,
      "step": 700
    },
    {
      "epoch": 0.10715363718683972,
      "grad_norm": 3.477902889251709,
      "learning_rate": 0.00038166666666666666,
      "loss": 0.8988,
      "step": 710
    },
    {
      "epoch": 0.10866284334440085,
      "grad_norm": 3.872215747833252,
      "learning_rate": 0.00038,
      "loss": 0.9928,
      "step": 720
    },
    {
      "epoch": 0.11017204950196197,
      "grad_norm": 4.986629009246826,
      "learning_rate": 0.0003783333333333334,
      "loss": 1.033,
      "step": 730
    },
    {
      "epoch": 0.1116812556595231,
      "grad_norm": 4.273701190948486,
      "learning_rate": 0.00037666666666666664,
      "loss": 0.6122,
      "step": 740
    },
    {
      "epoch": 0.11319046181708421,
      "grad_norm": 4.513174057006836,
      "learning_rate": 0.000375,
      "loss": 0.7713,
      "step": 750
    },
    {
      "epoch": 0.11469966797464534,
      "grad_norm": 3.188779592514038,
      "learning_rate": 0.0003733333333333334,
      "loss": 0.8896,
      "step": 760
    },
    {
      "epoch": 0.11620887413220646,
      "grad_norm": 4.035792350769043,
      "learning_rate": 0.00037166666666666663,
      "loss": 1.2484,
      "step": 770
    },
    {
      "epoch": 0.11771808028976759,
      "grad_norm": 10.202920913696289,
      "learning_rate": 0.00037,
      "loss": 1.0389,
      "step": 780
    },
    {
      "epoch": 0.1192272864473287,
      "grad_norm": 2.457153797149658,
      "learning_rate": 0.00036833333333333336,
      "loss": 0.9785,
      "step": 790
    },
    {
      "epoch": 0.12073649260488983,
      "grad_norm": 4.774085998535156,
      "learning_rate": 0.00036666666666666667,
      "loss": 1.0242,
      "step": 800
    },
    {
      "epoch": 0.12224569876245095,
      "grad_norm": 3.678788661956787,
      "learning_rate": 0.000365,
      "loss": 0.835,
      "step": 810
    },
    {
      "epoch": 0.12375490492001208,
      "grad_norm": 5.054089546203613,
      "learning_rate": 0.00036333333333333335,
      "loss": 1.0594,
      "step": 820
    },
    {
      "epoch": 0.1252641110775732,
      "grad_norm": 3.889106273651123,
      "learning_rate": 0.0003616666666666667,
      "loss": 0.9359,
      "step": 830
    },
    {
      "epoch": 0.12677331723513433,
      "grad_norm": 2.1147186756134033,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.9824,
      "step": 840
    },
    {
      "epoch": 0.12828252339269544,
      "grad_norm": 1.5495456457138062,
      "learning_rate": 0.00035833333333333333,
      "loss": 0.8258,
      "step": 850
    },
    {
      "epoch": 0.12979172955025656,
      "grad_norm": 3.3263347148895264,
      "learning_rate": 0.0003566666666666667,
      "loss": 0.7271,
      "step": 860
    },
    {
      "epoch": 0.1313009357078177,
      "grad_norm": 2.242946147918701,
      "learning_rate": 0.000355,
      "loss": 0.9539,
      "step": 870
    },
    {
      "epoch": 0.13281014186537882,
      "grad_norm": 4.038971424102783,
      "learning_rate": 0.0003533333333333333,
      "loss": 0.9602,
      "step": 880
    },
    {
      "epoch": 0.13431934802293993,
      "grad_norm": 1.8458564281463623,
      "learning_rate": 0.0003516666666666667,
      "loss": 0.9527,
      "step": 890
    },
    {
      "epoch": 0.13582855418050105,
      "grad_norm": 1.9671680927276611,
      "learning_rate": 0.00035,
      "loss": 0.9244,
      "step": 900
    },
    {
      "epoch": 0.1373377603380622,
      "grad_norm": 2.2189879417419434,
      "learning_rate": 0.00034833333333333336,
      "loss": 1.0648,
      "step": 910
    },
    {
      "epoch": 0.1388469664956233,
      "grad_norm": 4.01246976852417,
      "learning_rate": 0.00034666666666666667,
      "loss": 0.9725,
      "step": 920
    },
    {
      "epoch": 0.14035617265318442,
      "grad_norm": 5.658843517303467,
      "learning_rate": 0.000345,
      "loss": 0.8758,
      "step": 930
    },
    {
      "epoch": 0.14186537881074554,
      "grad_norm": 3.216097354888916,
      "learning_rate": 0.00034333333333333335,
      "loss": 0.8426,
      "step": 940
    },
    {
      "epoch": 0.14337458496830668,
      "grad_norm": 8.631975173950195,
      "learning_rate": 0.00034166666666666666,
      "loss": 1.1592,
      "step": 950
    },
    {
      "epoch": 0.1448837911258678,
      "grad_norm": 7.295071125030518,
      "learning_rate": 0.00034,
      "loss": 1.0805,
      "step": 960
    },
    {
      "epoch": 0.14639299728342892,
      "grad_norm": 3.719780445098877,
      "learning_rate": 0.00033833333333333334,
      "loss": 0.8109,
      "step": 970
    },
    {
      "epoch": 0.14790220344099003,
      "grad_norm": 2.8309264183044434,
      "learning_rate": 0.0003366666666666667,
      "loss": 0.7867,
      "step": 980
    },
    {
      "epoch": 0.14941140959855118,
      "grad_norm": 4.372933387756348,
      "learning_rate": 0.000335,
      "loss": 0.8026,
      "step": 990
    },
    {
      "epoch": 0.1509206157561123,
      "grad_norm": 3.0399832725524902,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.9305,
      "step": 1000
    },
    {
      "epoch": 0.1509206157561123,
      "eval_bleu-4": 0.05020789569194602,
      "eval_rouge-1": 35.49193680506686,
      "eval_rouge-2": 30.903936242083045,
      "eval_rouge-l": 8.765215974665729,
      "eval_runtime": 708.3433,
      "eval_samples_per_second": 2.006,
      "eval_steps_per_second": 0.503,
      "step": 1000
    },
    {
      "epoch": 0.1524298219136734,
      "grad_norm": 2.73970627784729,
      "learning_rate": 0.0003316666666666667,
      "loss": 0.8727,
      "step": 1010
    },
    {
      "epoch": 0.15393902807123452,
      "grad_norm": 4.839251518249512,
      "learning_rate": 0.00033,
      "loss": 0.9695,
      "step": 1020
    },
    {
      "epoch": 0.15544823422879564,
      "grad_norm": 2.032839059829712,
      "learning_rate": 0.0003283333333333333,
      "loss": 0.7829,
      "step": 1030
    },
    {
      "epoch": 0.15695744038635678,
      "grad_norm": 2.340562105178833,
      "learning_rate": 0.0003266666666666667,
      "loss": 0.9313,
      "step": 1040
    },
    {
      "epoch": 0.1584666465439179,
      "grad_norm": 6.55220890045166,
      "learning_rate": 0.00032500000000000004,
      "loss": 0.8314,
      "step": 1050
    },
    {
      "epoch": 0.15997585270147902,
      "grad_norm": 2.9905736446380615,
      "learning_rate": 0.0003233333333333333,
      "loss": 1.009,
      "step": 1060
    },
    {
      "epoch": 0.16148505885904013,
      "grad_norm": 2.4336998462677,
      "learning_rate": 0.00032166666666666666,
      "loss": 1.177,
      "step": 1070
    },
    {
      "epoch": 0.16299426501660128,
      "grad_norm": 2.3676178455352783,
      "learning_rate": 0.00032,
      "loss": 0.7383,
      "step": 1080
    },
    {
      "epoch": 0.1645034711741624,
      "grad_norm": 2.8367252349853516,
      "learning_rate": 0.00031833333333333334,
      "loss": 0.8297,
      "step": 1090
    },
    {
      "epoch": 0.1660126773317235,
      "grad_norm": 3.8272740840911865,
      "learning_rate": 0.00031666666666666665,
      "loss": 0.9082,
      "step": 1100
    },
    {
      "epoch": 0.16752188348928462,
      "grad_norm": 3.102463483810425,
      "learning_rate": 0.000315,
      "loss": 0.7207,
      "step": 1110
    },
    {
      "epoch": 0.16903108964684577,
      "grad_norm": 2.6532726287841797,
      "learning_rate": 0.0003133333333333334,
      "loss": 0.7039,
      "step": 1120
    },
    {
      "epoch": 0.17054029580440688,
      "grad_norm": 5.731143951416016,
      "learning_rate": 0.00031166666666666663,
      "loss": 1.1406,
      "step": 1130
    },
    {
      "epoch": 0.172049501961968,
      "grad_norm": 2.550562620162964,
      "learning_rate": 0.00031,
      "loss": 0.9004,
      "step": 1140
    },
    {
      "epoch": 0.17355870811952911,
      "grad_norm": 3.1832468509674072,
      "learning_rate": 0.00030833333333333337,
      "loss": 0.8332,
      "step": 1150
    },
    {
      "epoch": 0.17506791427709026,
      "grad_norm": 1.842581868171692,
      "learning_rate": 0.0003066666666666667,
      "loss": 0.8317,
      "step": 1160
    },
    {
      "epoch": 0.17657712043465137,
      "grad_norm": 2.833559036254883,
      "learning_rate": 0.000305,
      "loss": 0.8371,
      "step": 1170
    },
    {
      "epoch": 0.1780863265922125,
      "grad_norm": 3.4917609691619873,
      "learning_rate": 0.00030333333333333335,
      "loss": 0.6623,
      "step": 1180
    },
    {
      "epoch": 0.1795955327497736,
      "grad_norm": 7.062525749206543,
      "learning_rate": 0.0003016666666666667,
      "loss": 0.8443,
      "step": 1190
    },
    {
      "epoch": 0.18110473890733475,
      "grad_norm": 2.9331159591674805,
      "learning_rate": 0.0003,
      "loss": 0.7674,
      "step": 1200
    },
    {
      "epoch": 0.18261394506489587,
      "grad_norm": 4.304566860198975,
      "learning_rate": 0.00029833333333333334,
      "loss": 1.0373,
      "step": 1210
    },
    {
      "epoch": 0.18412315122245698,
      "grad_norm": 2.383122205734253,
      "learning_rate": 0.0002966666666666667,
      "loss": 0.7457,
      "step": 1220
    },
    {
      "epoch": 0.1856323573800181,
      "grad_norm": 6.031886577606201,
      "learning_rate": 0.000295,
      "loss": 0.7665,
      "step": 1230
    },
    {
      "epoch": 0.18714156353757924,
      "grad_norm": 5.535236358642578,
      "learning_rate": 0.0002933333333333333,
      "loss": 0.7117,
      "step": 1240
    },
    {
      "epoch": 0.18865076969514036,
      "grad_norm": 7.68691873550415,
      "learning_rate": 0.0002916666666666667,
      "loss": 1.034,
      "step": 1250
    },
    {
      "epoch": 0.19015997585270147,
      "grad_norm": 4.150859355926514,
      "learning_rate": 0.00029,
      "loss": 0.86,
      "step": 1260
    },
    {
      "epoch": 0.1916691820102626,
      "grad_norm": 3.401397228240967,
      "learning_rate": 0.0002883333333333333,
      "loss": 0.8777,
      "step": 1270
    },
    {
      "epoch": 0.19317838816782373,
      "grad_norm": 3.0845351219177246,
      "learning_rate": 0.0002866666666666667,
      "loss": 0.8119,
      "step": 1280
    },
    {
      "epoch": 0.19468759432538485,
      "grad_norm": 5.24231481552124,
      "learning_rate": 0.000285,
      "loss": 0.8617,
      "step": 1290
    },
    {
      "epoch": 0.19619680048294597,
      "grad_norm": 3.6776437759399414,
      "learning_rate": 0.00028333333333333335,
      "loss": 1.1234,
      "step": 1300
    },
    {
      "epoch": 0.19770600664050708,
      "grad_norm": 4.64456033706665,
      "learning_rate": 0.00028166666666666666,
      "loss": 0.859,
      "step": 1310
    },
    {
      "epoch": 0.19921521279806823,
      "grad_norm": 5.404375076293945,
      "learning_rate": 0.00028000000000000003,
      "loss": 1.2625,
      "step": 1320
    },
    {
      "epoch": 0.20072441895562934,
      "grad_norm": 2.7439818382263184,
      "learning_rate": 0.00027833333333333334,
      "loss": 0.7533,
      "step": 1330
    },
    {
      "epoch": 0.20223362511319046,
      "grad_norm": 3.336230754852295,
      "learning_rate": 0.00027666666666666665,
      "loss": 0.8441,
      "step": 1340
    },
    {
      "epoch": 0.20374283127075157,
      "grad_norm": 1.3307850360870361,
      "learning_rate": 0.000275,
      "loss": 0.8055,
      "step": 1350
    },
    {
      "epoch": 0.20525203742831272,
      "grad_norm": 7.027289867401123,
      "learning_rate": 0.00027333333333333333,
      "loss": 1.0152,
      "step": 1360
    },
    {
      "epoch": 0.20676124358587383,
      "grad_norm": 4.152199745178223,
      "learning_rate": 0.0002716666666666667,
      "loss": 0.6127,
      "step": 1370
    },
    {
      "epoch": 0.20827044974343495,
      "grad_norm": 2.739361047744751,
      "learning_rate": 0.00027,
      "loss": 1.1223,
      "step": 1380
    },
    {
      "epoch": 0.20977965590099606,
      "grad_norm": 5.959831714630127,
      "learning_rate": 0.0002683333333333333,
      "loss": 1.0852,
      "step": 1390
    },
    {
      "epoch": 0.2112888620585572,
      "grad_norm": 3.250722885131836,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.579,
      "step": 1400
    },
    {
      "epoch": 0.21279806821611832,
      "grad_norm": 4.340911388397217,
      "learning_rate": 0.00026500000000000004,
      "loss": 0.8104,
      "step": 1410
    },
    {
      "epoch": 0.21430727437367944,
      "grad_norm": 8.583422660827637,
      "learning_rate": 0.0002633333333333333,
      "loss": 0.8902,
      "step": 1420
    },
    {
      "epoch": 0.21581648053124056,
      "grad_norm": 6.590185642242432,
      "learning_rate": 0.00026166666666666667,
      "loss": 1.0371,
      "step": 1430
    },
    {
      "epoch": 0.2173256866888017,
      "grad_norm": 6.883553981781006,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.8359,
      "step": 1440
    },
    {
      "epoch": 0.21883489284636282,
      "grad_norm": 3.4606592655181885,
      "learning_rate": 0.00025833333333333334,
      "loss": 0.8867,
      "step": 1450
    },
    {
      "epoch": 0.22034409900392393,
      "grad_norm": 1.3614557981491089,
      "learning_rate": 0.00025666666666666665,
      "loss": 0.8283,
      "step": 1460
    },
    {
      "epoch": 0.22185330516148505,
      "grad_norm": 6.791159152984619,
      "learning_rate": 0.000255,
      "loss": 0.9875,
      "step": 1470
    },
    {
      "epoch": 0.2233625113190462,
      "grad_norm": 3.120854139328003,
      "learning_rate": 0.0002533333333333334,
      "loss": 0.6701,
      "step": 1480
    },
    {
      "epoch": 0.2248717174766073,
      "grad_norm": 3.5517578125,
      "learning_rate": 0.00025166666666666664,
      "loss": 0.6988,
      "step": 1490
    },
    {
      "epoch": 0.22638092363416842,
      "grad_norm": 1.938632607460022,
      "learning_rate": 0.00025,
      "loss": 1.0182,
      "step": 1500
    },
    {
      "epoch": 0.22638092363416842,
      "eval_bleu-4": 0.0467750399898948,
      "eval_rouge-1": 27.106196199859255,
      "eval_rouge-2": 26.403192892329347,
      "eval_rouge-l": 8.293039057002112,
      "eval_runtime": 456.2118,
      "eval_samples_per_second": 3.115,
      "eval_steps_per_second": 0.78,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1588735471419392e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
