{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.45276184726833685,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001509206157561123,
      "grad_norm": 4.526779651641846,
      "learning_rate": 0.0004983333333333334,
      "loss": 1.7812,
      "step": 10
    },
    {
      "epoch": 0.003018412315122246,
      "grad_norm": 3.6299264430999756,
      "learning_rate": 0.0004966666666666666,
      "loss": 1.2615,
      "step": 20
    },
    {
      "epoch": 0.004527618472683369,
      "grad_norm": 2.523728370666504,
      "learning_rate": 0.000495,
      "loss": 0.8381,
      "step": 30
    },
    {
      "epoch": 0.006036824630244492,
      "grad_norm": 4.617517948150635,
      "learning_rate": 0.0004933333333333334,
      "loss": 1.1124,
      "step": 40
    },
    {
      "epoch": 0.007546030787805615,
      "grad_norm": 4.280418872833252,
      "learning_rate": 0.0004916666666666666,
      "loss": 1.0023,
      "step": 50
    },
    {
      "epoch": 0.009055236945366738,
      "grad_norm": 4.2745842933654785,
      "learning_rate": 0.00049,
      "loss": 1.1932,
      "step": 60
    },
    {
      "epoch": 0.01056444310292786,
      "grad_norm": 3.2863259315490723,
      "learning_rate": 0.0004883333333333333,
      "loss": 1.0693,
      "step": 70
    },
    {
      "epoch": 0.012073649260488983,
      "grad_norm": 4.149944305419922,
      "learning_rate": 0.0004866666666666667,
      "loss": 1.0359,
      "step": 80
    },
    {
      "epoch": 0.013582855418050106,
      "grad_norm": 5.123433589935303,
      "learning_rate": 0.00048499999999999997,
      "loss": 1.15,
      "step": 90
    },
    {
      "epoch": 0.01509206157561123,
      "grad_norm": 2.4548261165618896,
      "learning_rate": 0.00048333333333333334,
      "loss": 0.9322,
      "step": 100
    },
    {
      "epoch": 0.016601267733172352,
      "grad_norm": 2.6163525581359863,
      "learning_rate": 0.0004816666666666667,
      "loss": 1.052,
      "step": 110
    },
    {
      "epoch": 0.018110473890733475,
      "grad_norm": 3.650675058364868,
      "learning_rate": 0.00048,
      "loss": 1.4148,
      "step": 120
    },
    {
      "epoch": 0.019619680048294598,
      "grad_norm": 4.163342475891113,
      "learning_rate": 0.0004783333333333333,
      "loss": 1.107,
      "step": 130
    },
    {
      "epoch": 0.02112888620585572,
      "grad_norm": 2.6465706825256348,
      "learning_rate": 0.0004766666666666667,
      "loss": 0.9945,
      "step": 140
    },
    {
      "epoch": 0.022638092363416844,
      "grad_norm": 4.102530002593994,
      "learning_rate": 0.000475,
      "loss": 0.784,
      "step": 150
    },
    {
      "epoch": 0.024147298520977967,
      "grad_norm": 5.171638011932373,
      "learning_rate": 0.00047333333333333336,
      "loss": 1.2555,
      "step": 160
    },
    {
      "epoch": 0.02565650467853909,
      "grad_norm": 4.003929615020752,
      "learning_rate": 0.0004716666666666667,
      "loss": 0.9605,
      "step": 170
    },
    {
      "epoch": 0.027165710836100213,
      "grad_norm": 4.162509918212891,
      "learning_rate": 0.00047,
      "loss": 1.0895,
      "step": 180
    },
    {
      "epoch": 0.028674916993661335,
      "grad_norm": 5.338866233825684,
      "learning_rate": 0.00046833333333333335,
      "loss": 1.152,
      "step": 190
    },
    {
      "epoch": 0.03018412315122246,
      "grad_norm": 3.918215274810791,
      "learning_rate": 0.00046666666666666666,
      "loss": 1.0471,
      "step": 200
    },
    {
      "epoch": 0.03169332930878358,
      "grad_norm": 2.553309440612793,
      "learning_rate": 0.000465,
      "loss": 1.2008,
      "step": 210
    },
    {
      "epoch": 0.033202535466344704,
      "grad_norm": 5.488369464874268,
      "learning_rate": 0.00046333333333333334,
      "loss": 1.3684,
      "step": 220
    },
    {
      "epoch": 0.03471174162390583,
      "grad_norm": 3.3953678607940674,
      "learning_rate": 0.0004616666666666667,
      "loss": 1.0859,
      "step": 230
    },
    {
      "epoch": 0.03622094778146695,
      "grad_norm": 5.445623874664307,
      "learning_rate": 0.00046,
      "loss": 0.8508,
      "step": 240
    },
    {
      "epoch": 0.03773015393902807,
      "grad_norm": 2.3504083156585693,
      "learning_rate": 0.0004583333333333333,
      "loss": 1.0516,
      "step": 250
    },
    {
      "epoch": 0.039239360096589196,
      "grad_norm": 8.683120727539062,
      "learning_rate": 0.0004566666666666667,
      "loss": 1.1355,
      "step": 260
    },
    {
      "epoch": 0.04074856625415032,
      "grad_norm": 6.5666961669921875,
      "learning_rate": 0.000455,
      "loss": 0.9324,
      "step": 270
    },
    {
      "epoch": 0.04225777241171144,
      "grad_norm": 8.30313491821289,
      "learning_rate": 0.0004533333333333333,
      "loss": 1.1682,
      "step": 280
    },
    {
      "epoch": 0.043766978569272565,
      "grad_norm": 2.2594666481018066,
      "learning_rate": 0.0004516666666666667,
      "loss": 1.2047,
      "step": 290
    },
    {
      "epoch": 0.04527618472683369,
      "grad_norm": 4.166910171508789,
      "learning_rate": 0.00045000000000000004,
      "loss": 1.1027,
      "step": 300
    },
    {
      "epoch": 0.04678539088439481,
      "grad_norm": 3.9617397785186768,
      "learning_rate": 0.0004483333333333333,
      "loss": 1.0277,
      "step": 310
    },
    {
      "epoch": 0.04829459704195593,
      "grad_norm": 3.49330997467041,
      "learning_rate": 0.00044666666666666666,
      "loss": 1.0688,
      "step": 320
    },
    {
      "epoch": 0.049803803199517056,
      "grad_norm": 2.9284489154815674,
      "learning_rate": 0.00044500000000000003,
      "loss": 1.0012,
      "step": 330
    },
    {
      "epoch": 0.05131300935707818,
      "grad_norm": 2.4470107555389404,
      "learning_rate": 0.00044333333333333334,
      "loss": 1.0434,
      "step": 340
    },
    {
      "epoch": 0.0528222155146393,
      "grad_norm": 5.3538641929626465,
      "learning_rate": 0.00044166666666666665,
      "loss": 0.9137,
      "step": 350
    },
    {
      "epoch": 0.054331421672200425,
      "grad_norm": 4.424758434295654,
      "learning_rate": 0.00044,
      "loss": 1.0152,
      "step": 360
    },
    {
      "epoch": 0.05584062782976155,
      "grad_norm": 2.2221152782440186,
      "learning_rate": 0.0004383333333333334,
      "loss": 1.0074,
      "step": 370
    },
    {
      "epoch": 0.05734983398732267,
      "grad_norm": 8.342182159423828,
      "learning_rate": 0.00043666666666666664,
      "loss": 0.8652,
      "step": 380
    },
    {
      "epoch": 0.058859040144883794,
      "grad_norm": 4.247380256652832,
      "learning_rate": 0.000435,
      "loss": 0.9426,
      "step": 390
    },
    {
      "epoch": 0.06036824630244492,
      "grad_norm": 3.9363439083099365,
      "learning_rate": 0.00043333333333333337,
      "loss": 0.7859,
      "step": 400
    },
    {
      "epoch": 0.06187745246000604,
      "grad_norm": 3.390939950942993,
      "learning_rate": 0.0004316666666666667,
      "loss": 0.9373,
      "step": 410
    },
    {
      "epoch": 0.06338665861756716,
      "grad_norm": 1.9428819417953491,
      "learning_rate": 0.00043,
      "loss": 1.0242,
      "step": 420
    },
    {
      "epoch": 0.06489586477512828,
      "grad_norm": 2.896209955215454,
      "learning_rate": 0.00042833333333333335,
      "loss": 0.8498,
      "step": 430
    },
    {
      "epoch": 0.06640507093268941,
      "grad_norm": 5.313353538513184,
      "learning_rate": 0.0004266666666666667,
      "loss": 0.9557,
      "step": 440
    },
    {
      "epoch": 0.06791427709025052,
      "grad_norm": 4.082955360412598,
      "learning_rate": 0.000425,
      "loss": 0.9406,
      "step": 450
    },
    {
      "epoch": 0.06942348324781165,
      "grad_norm": 4.587948322296143,
      "learning_rate": 0.00042333333333333334,
      "loss": 0.8439,
      "step": 460
    },
    {
      "epoch": 0.07093268940537277,
      "grad_norm": 4.431200981140137,
      "learning_rate": 0.0004216666666666667,
      "loss": 1.1678,
      "step": 470
    },
    {
      "epoch": 0.0724418955629339,
      "grad_norm": 3.31732439994812,
      "learning_rate": 0.00042,
      "loss": 0.8273,
      "step": 480
    },
    {
      "epoch": 0.07395110172049502,
      "grad_norm": 1.845638632774353,
      "learning_rate": 0.00041833333333333333,
      "loss": 0.8957,
      "step": 490
    },
    {
      "epoch": 0.07546030787805615,
      "grad_norm": 1.3633835315704346,
      "learning_rate": 0.0004166666666666667,
      "loss": 0.9088,
      "step": 500
    },
    {
      "epoch": 0.07546030787805615,
      "eval_bleu-4": 0.046722778375846534,
      "eval_rouge-1": 27.196036875439834,
      "eval_rouge-2": 25.658205770584097,
      "eval_rouge-l": 8.26850239268121,
      "eval_runtime": 502.6535,
      "eval_samples_per_second": 2.827,
      "eval_steps_per_second": 0.708,
      "step": 500
    },
    {
      "epoch": 0.07696951403561726,
      "grad_norm": 3.836848020553589,
      "learning_rate": 0.000415,
      "loss": 1.2309,
      "step": 510
    },
    {
      "epoch": 0.07847872019317839,
      "grad_norm": 1.6586923599243164,
      "learning_rate": 0.0004133333333333333,
      "loss": 0.8529,
      "step": 520
    },
    {
      "epoch": 0.07998792635073951,
      "grad_norm": 2.31618332862854,
      "learning_rate": 0.0004116666666666667,
      "loss": 0.9672,
      "step": 530
    },
    {
      "epoch": 0.08149713250830064,
      "grad_norm": 5.9583330154418945,
      "learning_rate": 0.00041,
      "loss": 1.1178,
      "step": 540
    },
    {
      "epoch": 0.08300633866586175,
      "grad_norm": 3.986414670944214,
      "learning_rate": 0.00040833333333333336,
      "loss": 0.8846,
      "step": 550
    },
    {
      "epoch": 0.08451554482342288,
      "grad_norm": 9.42641544342041,
      "learning_rate": 0.00040666666666666667,
      "loss": 1.0516,
      "step": 560
    },
    {
      "epoch": 0.086024750980984,
      "grad_norm": 4.890380382537842,
      "learning_rate": 0.00040500000000000003,
      "loss": 1.098,
      "step": 570
    },
    {
      "epoch": 0.08753395713854513,
      "grad_norm": 6.560605525970459,
      "learning_rate": 0.00040333333333333334,
      "loss": 0.9654,
      "step": 580
    },
    {
      "epoch": 0.08904316329610625,
      "grad_norm": 3.294018030166626,
      "learning_rate": 0.00040166666666666665,
      "loss": 0.9832,
      "step": 590
    },
    {
      "epoch": 0.09055236945366738,
      "grad_norm": 2.524383306503296,
      "learning_rate": 0.0004,
      "loss": 0.9836,
      "step": 600
    },
    {
      "epoch": 0.09206157561122849,
      "grad_norm": 3.1457579135894775,
      "learning_rate": 0.00039833333333333333,
      "loss": 1.2008,
      "step": 610
    },
    {
      "epoch": 0.09357078176878962,
      "grad_norm": 4.523575782775879,
      "learning_rate": 0.0003966666666666667,
      "loss": 1.0,
      "step": 620
    },
    {
      "epoch": 0.09507998792635074,
      "grad_norm": 3.7047181129455566,
      "learning_rate": 0.000395,
      "loss": 0.9293,
      "step": 630
    },
    {
      "epoch": 0.09658919408391187,
      "grad_norm": 1.5108058452606201,
      "learning_rate": 0.0003933333333333333,
      "loss": 0.8359,
      "step": 640
    },
    {
      "epoch": 0.09809840024147298,
      "grad_norm": 3.422895908355713,
      "learning_rate": 0.0003916666666666667,
      "loss": 0.9664,
      "step": 650
    },
    {
      "epoch": 0.09960760639903411,
      "grad_norm": 3.7136294841766357,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.8592,
      "step": 660
    },
    {
      "epoch": 0.10111681255659523,
      "grad_norm": 2.342552423477173,
      "learning_rate": 0.0003883333333333333,
      "loss": 0.891,
      "step": 670
    },
    {
      "epoch": 0.10262601871415636,
      "grad_norm": 3.4248383045196533,
      "learning_rate": 0.00038666666666666667,
      "loss": 0.8959,
      "step": 680
    },
    {
      "epoch": 0.10413522487171747,
      "grad_norm": 2.955563545227051,
      "learning_rate": 0.00038500000000000003,
      "loss": 1.0521,
      "step": 690
    },
    {
      "epoch": 0.1056444310292786,
      "grad_norm": 1.354830026626587,
      "learning_rate": 0.00038333333333333334,
      "loss": 0.9434,
      "step": 700
    },
    {
      "epoch": 0.10715363718683972,
      "grad_norm": 3.477902889251709,
      "learning_rate": 0.00038166666666666666,
      "loss": 0.8988,
      "step": 710
    },
    {
      "epoch": 0.10866284334440085,
      "grad_norm": 3.872215747833252,
      "learning_rate": 0.00038,
      "loss": 0.9928,
      "step": 720
    },
    {
      "epoch": 0.11017204950196197,
      "grad_norm": 4.986629009246826,
      "learning_rate": 0.0003783333333333334,
      "loss": 1.033,
      "step": 730
    },
    {
      "epoch": 0.1116812556595231,
      "grad_norm": 4.273701190948486,
      "learning_rate": 0.00037666666666666664,
      "loss": 0.6122,
      "step": 740
    },
    {
      "epoch": 0.11319046181708421,
      "grad_norm": 4.513174057006836,
      "learning_rate": 0.000375,
      "loss": 0.7713,
      "step": 750
    },
    {
      "epoch": 0.11469966797464534,
      "grad_norm": 3.188779592514038,
      "learning_rate": 0.0003733333333333334,
      "loss": 0.8896,
      "step": 760
    },
    {
      "epoch": 0.11620887413220646,
      "grad_norm": 4.035792350769043,
      "learning_rate": 0.00037166666666666663,
      "loss": 1.2484,
      "step": 770
    },
    {
      "epoch": 0.11771808028976759,
      "grad_norm": 10.202920913696289,
      "learning_rate": 0.00037,
      "loss": 1.0389,
      "step": 780
    },
    {
      "epoch": 0.1192272864473287,
      "grad_norm": 2.457153797149658,
      "learning_rate": 0.00036833333333333336,
      "loss": 0.9785,
      "step": 790
    },
    {
      "epoch": 0.12073649260488983,
      "grad_norm": 4.774085998535156,
      "learning_rate": 0.00036666666666666667,
      "loss": 1.0242,
      "step": 800
    },
    {
      "epoch": 0.12224569876245095,
      "grad_norm": 3.678788661956787,
      "learning_rate": 0.000365,
      "loss": 0.835,
      "step": 810
    },
    {
      "epoch": 0.12375490492001208,
      "grad_norm": 5.054089546203613,
      "learning_rate": 0.00036333333333333335,
      "loss": 1.0594,
      "step": 820
    },
    {
      "epoch": 0.1252641110775732,
      "grad_norm": 3.889106273651123,
      "learning_rate": 0.0003616666666666667,
      "loss": 0.9359,
      "step": 830
    },
    {
      "epoch": 0.12677331723513433,
      "grad_norm": 2.1147186756134033,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.9824,
      "step": 840
    },
    {
      "epoch": 0.12828252339269544,
      "grad_norm": 1.5495456457138062,
      "learning_rate": 0.00035833333333333333,
      "loss": 0.8258,
      "step": 850
    },
    {
      "epoch": 0.12979172955025656,
      "grad_norm": 3.3263347148895264,
      "learning_rate": 0.0003566666666666667,
      "loss": 0.7271,
      "step": 860
    },
    {
      "epoch": 0.1313009357078177,
      "grad_norm": 2.242946147918701,
      "learning_rate": 0.000355,
      "loss": 0.9539,
      "step": 870
    },
    {
      "epoch": 0.13281014186537882,
      "grad_norm": 4.038971424102783,
      "learning_rate": 0.0003533333333333333,
      "loss": 0.9602,
      "step": 880
    },
    {
      "epoch": 0.13431934802293993,
      "grad_norm": 1.8458564281463623,
      "learning_rate": 0.0003516666666666667,
      "loss": 0.9527,
      "step": 890
    },
    {
      "epoch": 0.13582855418050105,
      "grad_norm": 1.9671680927276611,
      "learning_rate": 0.00035,
      "loss": 0.9244,
      "step": 900
    },
    {
      "epoch": 0.1373377603380622,
      "grad_norm": 2.2189879417419434,
      "learning_rate": 0.00034833333333333336,
      "loss": 1.0648,
      "step": 910
    },
    {
      "epoch": 0.1388469664956233,
      "grad_norm": 4.01246976852417,
      "learning_rate": 0.00034666666666666667,
      "loss": 0.9725,
      "step": 920
    },
    {
      "epoch": 0.14035617265318442,
      "grad_norm": 5.658843517303467,
      "learning_rate": 0.000345,
      "loss": 0.8758,
      "step": 930
    },
    {
      "epoch": 0.14186537881074554,
      "grad_norm": 3.216097354888916,
      "learning_rate": 0.00034333333333333335,
      "loss": 0.8426,
      "step": 940
    },
    {
      "epoch": 0.14337458496830668,
      "grad_norm": 8.631975173950195,
      "learning_rate": 0.00034166666666666666,
      "loss": 1.1592,
      "step": 950
    },
    {
      "epoch": 0.1448837911258678,
      "grad_norm": 7.295071125030518,
      "learning_rate": 0.00034,
      "loss": 1.0805,
      "step": 960
    },
    {
      "epoch": 0.14639299728342892,
      "grad_norm": 3.719780445098877,
      "learning_rate": 0.00033833333333333334,
      "loss": 0.8109,
      "step": 970
    },
    {
      "epoch": 0.14790220344099003,
      "grad_norm": 2.8309264183044434,
      "learning_rate": 0.0003366666666666667,
      "loss": 0.7867,
      "step": 980
    },
    {
      "epoch": 0.14941140959855118,
      "grad_norm": 4.372933387756348,
      "learning_rate": 0.000335,
      "loss": 0.8026,
      "step": 990
    },
    {
      "epoch": 0.1509206157561123,
      "grad_norm": 3.0399832725524902,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.9305,
      "step": 1000
    },
    {
      "epoch": 0.1509206157561123,
      "eval_bleu-4": 0.05020789569194602,
      "eval_rouge-1": 35.49193680506686,
      "eval_rouge-2": 30.903936242083045,
      "eval_rouge-l": 8.765215974665729,
      "eval_runtime": 708.3433,
      "eval_samples_per_second": 2.006,
      "eval_steps_per_second": 0.503,
      "step": 1000
    },
    {
      "epoch": 0.1524298219136734,
      "grad_norm": 2.73970627784729,
      "learning_rate": 0.0003316666666666667,
      "loss": 0.8727,
      "step": 1010
    },
    {
      "epoch": 0.15393902807123452,
      "grad_norm": 4.839251518249512,
      "learning_rate": 0.00033,
      "loss": 0.9695,
      "step": 1020
    },
    {
      "epoch": 0.15544823422879564,
      "grad_norm": 2.032839059829712,
      "learning_rate": 0.0003283333333333333,
      "loss": 0.7829,
      "step": 1030
    },
    {
      "epoch": 0.15695744038635678,
      "grad_norm": 2.340562105178833,
      "learning_rate": 0.0003266666666666667,
      "loss": 0.9313,
      "step": 1040
    },
    {
      "epoch": 0.1584666465439179,
      "grad_norm": 6.55220890045166,
      "learning_rate": 0.00032500000000000004,
      "loss": 0.8314,
      "step": 1050
    },
    {
      "epoch": 0.15997585270147902,
      "grad_norm": 2.9905736446380615,
      "learning_rate": 0.0003233333333333333,
      "loss": 1.009,
      "step": 1060
    },
    {
      "epoch": 0.16148505885904013,
      "grad_norm": 2.4336998462677,
      "learning_rate": 0.00032166666666666666,
      "loss": 1.177,
      "step": 1070
    },
    {
      "epoch": 0.16299426501660128,
      "grad_norm": 2.3676178455352783,
      "learning_rate": 0.00032,
      "loss": 0.7383,
      "step": 1080
    },
    {
      "epoch": 0.1645034711741624,
      "grad_norm": 2.8367252349853516,
      "learning_rate": 0.00031833333333333334,
      "loss": 0.8297,
      "step": 1090
    },
    {
      "epoch": 0.1660126773317235,
      "grad_norm": 3.8272740840911865,
      "learning_rate": 0.00031666666666666665,
      "loss": 0.9082,
      "step": 1100
    },
    {
      "epoch": 0.16752188348928462,
      "grad_norm": 3.102463483810425,
      "learning_rate": 0.000315,
      "loss": 0.7207,
      "step": 1110
    },
    {
      "epoch": 0.16903108964684577,
      "grad_norm": 2.6532726287841797,
      "learning_rate": 0.0003133333333333334,
      "loss": 0.7039,
      "step": 1120
    },
    {
      "epoch": 0.17054029580440688,
      "grad_norm": 5.731143951416016,
      "learning_rate": 0.00031166666666666663,
      "loss": 1.1406,
      "step": 1130
    },
    {
      "epoch": 0.172049501961968,
      "grad_norm": 2.550562620162964,
      "learning_rate": 0.00031,
      "loss": 0.9004,
      "step": 1140
    },
    {
      "epoch": 0.17355870811952911,
      "grad_norm": 3.1832468509674072,
      "learning_rate": 0.00030833333333333337,
      "loss": 0.8332,
      "step": 1150
    },
    {
      "epoch": 0.17506791427709026,
      "grad_norm": 1.842581868171692,
      "learning_rate": 0.0003066666666666667,
      "loss": 0.8317,
      "step": 1160
    },
    {
      "epoch": 0.17657712043465137,
      "grad_norm": 2.833559036254883,
      "learning_rate": 0.000305,
      "loss": 0.8371,
      "step": 1170
    },
    {
      "epoch": 0.1780863265922125,
      "grad_norm": 3.4917609691619873,
      "learning_rate": 0.00030333333333333335,
      "loss": 0.6623,
      "step": 1180
    },
    {
      "epoch": 0.1795955327497736,
      "grad_norm": 7.062525749206543,
      "learning_rate": 0.0003016666666666667,
      "loss": 0.8443,
      "step": 1190
    },
    {
      "epoch": 0.18110473890733475,
      "grad_norm": 2.9331159591674805,
      "learning_rate": 0.0003,
      "loss": 0.7674,
      "step": 1200
    },
    {
      "epoch": 0.18261394506489587,
      "grad_norm": 4.304566860198975,
      "learning_rate": 0.00029833333333333334,
      "loss": 1.0373,
      "step": 1210
    },
    {
      "epoch": 0.18412315122245698,
      "grad_norm": 2.383122205734253,
      "learning_rate": 0.0002966666666666667,
      "loss": 0.7457,
      "step": 1220
    },
    {
      "epoch": 0.1856323573800181,
      "grad_norm": 6.031886577606201,
      "learning_rate": 0.000295,
      "loss": 0.7665,
      "step": 1230
    },
    {
      "epoch": 0.18714156353757924,
      "grad_norm": 5.535236358642578,
      "learning_rate": 0.0002933333333333333,
      "loss": 0.7117,
      "step": 1240
    },
    {
      "epoch": 0.18865076969514036,
      "grad_norm": 7.68691873550415,
      "learning_rate": 0.0002916666666666667,
      "loss": 1.034,
      "step": 1250
    },
    {
      "epoch": 0.19015997585270147,
      "grad_norm": 4.150859355926514,
      "learning_rate": 0.00029,
      "loss": 0.86,
      "step": 1260
    },
    {
      "epoch": 0.1916691820102626,
      "grad_norm": 3.401397228240967,
      "learning_rate": 0.0002883333333333333,
      "loss": 0.8777,
      "step": 1270
    },
    {
      "epoch": 0.19317838816782373,
      "grad_norm": 3.0845351219177246,
      "learning_rate": 0.0002866666666666667,
      "loss": 0.8119,
      "step": 1280
    },
    {
      "epoch": 0.19468759432538485,
      "grad_norm": 5.24231481552124,
      "learning_rate": 0.000285,
      "loss": 0.8617,
      "step": 1290
    },
    {
      "epoch": 0.19619680048294597,
      "grad_norm": 3.6776437759399414,
      "learning_rate": 0.00028333333333333335,
      "loss": 1.1234,
      "step": 1300
    },
    {
      "epoch": 0.19770600664050708,
      "grad_norm": 4.64456033706665,
      "learning_rate": 0.00028166666666666666,
      "loss": 0.859,
      "step": 1310
    },
    {
      "epoch": 0.19921521279806823,
      "grad_norm": 5.404375076293945,
      "learning_rate": 0.00028000000000000003,
      "loss": 1.2625,
      "step": 1320
    },
    {
      "epoch": 0.20072441895562934,
      "grad_norm": 2.7439818382263184,
      "learning_rate": 0.00027833333333333334,
      "loss": 0.7533,
      "step": 1330
    },
    {
      "epoch": 0.20223362511319046,
      "grad_norm": 3.336230754852295,
      "learning_rate": 0.00027666666666666665,
      "loss": 0.8441,
      "step": 1340
    },
    {
      "epoch": 0.20374283127075157,
      "grad_norm": 1.3307850360870361,
      "learning_rate": 0.000275,
      "loss": 0.8055,
      "step": 1350
    },
    {
      "epoch": 0.20525203742831272,
      "grad_norm": 7.027289867401123,
      "learning_rate": 0.00027333333333333333,
      "loss": 1.0152,
      "step": 1360
    },
    {
      "epoch": 0.20676124358587383,
      "grad_norm": 4.152199745178223,
      "learning_rate": 0.0002716666666666667,
      "loss": 0.6127,
      "step": 1370
    },
    {
      "epoch": 0.20827044974343495,
      "grad_norm": 2.739361047744751,
      "learning_rate": 0.00027,
      "loss": 1.1223,
      "step": 1380
    },
    {
      "epoch": 0.20977965590099606,
      "grad_norm": 5.959831714630127,
      "learning_rate": 0.0002683333333333333,
      "loss": 1.0852,
      "step": 1390
    },
    {
      "epoch": 0.2112888620585572,
      "grad_norm": 3.250722885131836,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.579,
      "step": 1400
    },
    {
      "epoch": 0.21279806821611832,
      "grad_norm": 4.340911388397217,
      "learning_rate": 0.00026500000000000004,
      "loss": 0.8104,
      "step": 1410
    },
    {
      "epoch": 0.21430727437367944,
      "grad_norm": 8.583422660827637,
      "learning_rate": 0.0002633333333333333,
      "loss": 0.8902,
      "step": 1420
    },
    {
      "epoch": 0.21581648053124056,
      "grad_norm": 6.590185642242432,
      "learning_rate": 0.00026166666666666667,
      "loss": 1.0371,
      "step": 1430
    },
    {
      "epoch": 0.2173256866888017,
      "grad_norm": 6.883553981781006,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.8359,
      "step": 1440
    },
    {
      "epoch": 0.21883489284636282,
      "grad_norm": 3.4606592655181885,
      "learning_rate": 0.00025833333333333334,
      "loss": 0.8867,
      "step": 1450
    },
    {
      "epoch": 0.22034409900392393,
      "grad_norm": 1.3614557981491089,
      "learning_rate": 0.00025666666666666665,
      "loss": 0.8283,
      "step": 1460
    },
    {
      "epoch": 0.22185330516148505,
      "grad_norm": 6.791159152984619,
      "learning_rate": 0.000255,
      "loss": 0.9875,
      "step": 1470
    },
    {
      "epoch": 0.2233625113190462,
      "grad_norm": 3.120854139328003,
      "learning_rate": 0.0002533333333333334,
      "loss": 0.6701,
      "step": 1480
    },
    {
      "epoch": 0.2248717174766073,
      "grad_norm": 3.5517578125,
      "learning_rate": 0.00025166666666666664,
      "loss": 0.6988,
      "step": 1490
    },
    {
      "epoch": 0.22638092363416842,
      "grad_norm": 1.938632607460022,
      "learning_rate": 0.00025,
      "loss": 1.0182,
      "step": 1500
    },
    {
      "epoch": 0.22638092363416842,
      "eval_bleu-4": 0.0467750399898948,
      "eval_rouge-1": 27.106196199859255,
      "eval_rouge-2": 26.403192892329347,
      "eval_rouge-l": 8.293039057002112,
      "eval_runtime": 456.2118,
      "eval_samples_per_second": 3.115,
      "eval_steps_per_second": 0.78,
      "step": 1500
    },
    {
      "epoch": 0.22789012979172954,
      "grad_norm": 3.237659454345703,
      "learning_rate": 0.0002483333333333333,
      "loss": 0.9885,
      "step": 1510
    },
    {
      "epoch": 0.22939933594929068,
      "grad_norm": 2.885751485824585,
      "learning_rate": 0.0002466666666666667,
      "loss": 0.8652,
      "step": 1520
    },
    {
      "epoch": 0.2309085421068518,
      "grad_norm": 5.539098262786865,
      "learning_rate": 0.000245,
      "loss": 0.9348,
      "step": 1530
    },
    {
      "epoch": 0.23241774826441292,
      "grad_norm": 3.2425665855407715,
      "learning_rate": 0.00024333333333333336,
      "loss": 0.8484,
      "step": 1540
    },
    {
      "epoch": 0.23392695442197403,
      "grad_norm": 3.52072811126709,
      "learning_rate": 0.00024166666666666667,
      "loss": 0.9617,
      "step": 1550
    },
    {
      "epoch": 0.23543616057953518,
      "grad_norm": 2.3295788764953613,
      "learning_rate": 0.00024,
      "loss": 0.7496,
      "step": 1560
    },
    {
      "epoch": 0.2369453667370963,
      "grad_norm": 6.2800703048706055,
      "learning_rate": 0.00023833333333333334,
      "loss": 0.9121,
      "step": 1570
    },
    {
      "epoch": 0.2384545728946574,
      "grad_norm": 4.127994537353516,
      "learning_rate": 0.00023666666666666668,
      "loss": 0.8336,
      "step": 1580
    },
    {
      "epoch": 0.23996377905221852,
      "grad_norm": 3.400944471359253,
      "learning_rate": 0.000235,
      "loss": 0.801,
      "step": 1590
    },
    {
      "epoch": 0.24147298520977967,
      "grad_norm": 3.858057737350464,
      "learning_rate": 0.00023333333333333333,
      "loss": 0.8275,
      "step": 1600
    },
    {
      "epoch": 0.24298219136734078,
      "grad_norm": 1.9275705814361572,
      "learning_rate": 0.00023166666666666667,
      "loss": 0.8253,
      "step": 1610
    },
    {
      "epoch": 0.2444913975249019,
      "grad_norm": 3.373385190963745,
      "learning_rate": 0.00023,
      "loss": 0.8262,
      "step": 1620
    },
    {
      "epoch": 0.24600060368246301,
      "grad_norm": 2.3416459560394287,
      "learning_rate": 0.00022833333333333334,
      "loss": 0.7904,
      "step": 1630
    },
    {
      "epoch": 0.24750980984002416,
      "grad_norm": 3.8086445331573486,
      "learning_rate": 0.00022666666666666666,
      "loss": 0.777,
      "step": 1640
    },
    {
      "epoch": 0.24901901599758527,
      "grad_norm": 6.224859237670898,
      "learning_rate": 0.00022500000000000002,
      "loss": 0.815,
      "step": 1650
    },
    {
      "epoch": 0.2505282221551464,
      "grad_norm": 4.941895484924316,
      "learning_rate": 0.00022333333333333333,
      "loss": 0.9145,
      "step": 1660
    },
    {
      "epoch": 0.25203742831270753,
      "grad_norm": 1.6894551515579224,
      "learning_rate": 0.00022166666666666667,
      "loss": 0.7785,
      "step": 1670
    },
    {
      "epoch": 0.25354663447026865,
      "grad_norm": 3.5521974563598633,
      "learning_rate": 0.00022,
      "loss": 0.8732,
      "step": 1680
    },
    {
      "epoch": 0.25505584062782977,
      "grad_norm": 2.351257085800171,
      "learning_rate": 0.00021833333333333332,
      "loss": 0.7675,
      "step": 1690
    },
    {
      "epoch": 0.2565650467853909,
      "grad_norm": 4.272441387176514,
      "learning_rate": 0.00021666666666666668,
      "loss": 0.8543,
      "step": 1700
    },
    {
      "epoch": 0.258074252942952,
      "grad_norm": 3.130815267562866,
      "learning_rate": 0.000215,
      "loss": 0.8453,
      "step": 1710
    },
    {
      "epoch": 0.2595834591005131,
      "grad_norm": 2.7826168537139893,
      "learning_rate": 0.00021333333333333336,
      "loss": 0.8336,
      "step": 1720
    },
    {
      "epoch": 0.26109266525807423,
      "grad_norm": 2.2991127967834473,
      "learning_rate": 0.00021166666666666667,
      "loss": 0.7975,
      "step": 1730
    },
    {
      "epoch": 0.2626018714156354,
      "grad_norm": 2.163691282272339,
      "learning_rate": 0.00021,
      "loss": 0.8647,
      "step": 1740
    },
    {
      "epoch": 0.2641110775731965,
      "grad_norm": 2.9375486373901367,
      "learning_rate": 0.00020833333333333335,
      "loss": 0.8445,
      "step": 1750
    },
    {
      "epoch": 0.26562028373075763,
      "grad_norm": 2.592883348464966,
      "learning_rate": 0.00020666666666666666,
      "loss": 0.8348,
      "step": 1760
    },
    {
      "epoch": 0.26712948988831875,
      "grad_norm": 1.857164978981018,
      "learning_rate": 0.000205,
      "loss": 0.7807,
      "step": 1770
    },
    {
      "epoch": 0.26863869604587987,
      "grad_norm": 7.0911078453063965,
      "learning_rate": 0.00020333333333333333,
      "loss": 0.8148,
      "step": 1780
    },
    {
      "epoch": 0.270147902203441,
      "grad_norm": 6.056526184082031,
      "learning_rate": 0.00020166666666666667,
      "loss": 0.8414,
      "step": 1790
    },
    {
      "epoch": 0.2716571083610021,
      "grad_norm": 3.6120688915252686,
      "learning_rate": 0.0002,
      "loss": 0.883,
      "step": 1800
    },
    {
      "epoch": 0.2731663145185632,
      "grad_norm": 1.971893310546875,
      "learning_rate": 0.00019833333333333335,
      "loss": 0.7098,
      "step": 1810
    },
    {
      "epoch": 0.2746755206761244,
      "grad_norm": 1.6938337087631226,
      "learning_rate": 0.00019666666666666666,
      "loss": 0.6301,
      "step": 1820
    },
    {
      "epoch": 0.2761847268336855,
      "grad_norm": 3.4982333183288574,
      "learning_rate": 0.00019500000000000002,
      "loss": 0.7643,
      "step": 1830
    },
    {
      "epoch": 0.2776939329912466,
      "grad_norm": 6.567866802215576,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.8861,
      "step": 1840
    },
    {
      "epoch": 0.27920313914880773,
      "grad_norm": 4.253896236419678,
      "learning_rate": 0.00019166666666666667,
      "loss": 0.8691,
      "step": 1850
    },
    {
      "epoch": 0.28071234530636885,
      "grad_norm": 4.850958824157715,
      "learning_rate": 0.00019,
      "loss": 0.7791,
      "step": 1860
    },
    {
      "epoch": 0.28222155146392996,
      "grad_norm": 7.6831560134887695,
      "learning_rate": 0.00018833333333333332,
      "loss": 1.0043,
      "step": 1870
    },
    {
      "epoch": 0.2837307576214911,
      "grad_norm": 2.72929310798645,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.8559,
      "step": 1880
    },
    {
      "epoch": 0.2852399637790522,
      "grad_norm": 6.1922736167907715,
      "learning_rate": 0.000185,
      "loss": 1.0895,
      "step": 1890
    },
    {
      "epoch": 0.28674916993661337,
      "grad_norm": 1.9894564151763916,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.9775,
      "step": 1900
    },
    {
      "epoch": 0.2882583760941745,
      "grad_norm": 4.358192443847656,
      "learning_rate": 0.00018166666666666667,
      "loss": 0.7897,
      "step": 1910
    },
    {
      "epoch": 0.2897675822517356,
      "grad_norm": 3.567953586578369,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.7361,
      "step": 1920
    },
    {
      "epoch": 0.2912767884092967,
      "grad_norm": 9.217056274414062,
      "learning_rate": 0.00017833333333333335,
      "loss": 0.7887,
      "step": 1930
    },
    {
      "epoch": 0.29278599456685783,
      "grad_norm": 2.5227622985839844,
      "learning_rate": 0.00017666666666666666,
      "loss": 0.7531,
      "step": 1940
    },
    {
      "epoch": 0.29429520072441895,
      "grad_norm": 1.7037544250488281,
      "learning_rate": 0.000175,
      "loss": 1.0754,
      "step": 1950
    },
    {
      "epoch": 0.29580440688198006,
      "grad_norm": 1.845988392829895,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.9564,
      "step": 1960
    },
    {
      "epoch": 0.2973136130395412,
      "grad_norm": 2.4044110774993896,
      "learning_rate": 0.00017166666666666667,
      "loss": 0.7605,
      "step": 1970
    },
    {
      "epoch": 0.29882281919710235,
      "grad_norm": 8.012447357177734,
      "learning_rate": 0.00017,
      "loss": 0.6517,
      "step": 1980
    },
    {
      "epoch": 0.30033202535466347,
      "grad_norm": 4.39015007019043,
      "learning_rate": 0.00016833333333333335,
      "loss": 0.9076,
      "step": 1990
    },
    {
      "epoch": 0.3018412315122246,
      "grad_norm": 3.2158634662628174,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.8049,
      "step": 2000
    },
    {
      "epoch": 0.3018412315122246,
      "eval_bleu-4": 0.0467343427721398,
      "eval_rouge-1": 27.94981527093596,
      "eval_rouge-2": 27.06132223786066,
      "eval_rouge-l": 8.278616819141451,
      "eval_runtime": 488.3624,
      "eval_samples_per_second": 2.91,
      "eval_steps_per_second": 0.729,
      "step": 2000
    },
    {
      "epoch": 0.3033504376697857,
      "grad_norm": 4.945282459259033,
      "learning_rate": 0.000165,
      "loss": 0.6935,
      "step": 2010
    },
    {
      "epoch": 0.3048596438273468,
      "grad_norm": 2.636687994003296,
      "learning_rate": 0.00016333333333333334,
      "loss": 0.8502,
      "step": 2020
    },
    {
      "epoch": 0.30636884998490793,
      "grad_norm": 6.588089942932129,
      "learning_rate": 0.00016166666666666665,
      "loss": 0.5792,
      "step": 2030
    },
    {
      "epoch": 0.30787805614246905,
      "grad_norm": 4.739089488983154,
      "learning_rate": 0.00016,
      "loss": 0.8047,
      "step": 2040
    },
    {
      "epoch": 0.30938726230003016,
      "grad_norm": 5.340967655181885,
      "learning_rate": 0.00015833333333333332,
      "loss": 0.8529,
      "step": 2050
    },
    {
      "epoch": 0.3108964684575913,
      "grad_norm": 4.527912616729736,
      "learning_rate": 0.0001566666666666667,
      "loss": 0.6434,
      "step": 2060
    },
    {
      "epoch": 0.31240567461515245,
      "grad_norm": 2.7592594623565674,
      "learning_rate": 0.000155,
      "loss": 0.8838,
      "step": 2070
    },
    {
      "epoch": 0.31391488077271357,
      "grad_norm": 3.7018630504608154,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.5842,
      "step": 2080
    },
    {
      "epoch": 0.3154240869302747,
      "grad_norm": 3.7400381565093994,
      "learning_rate": 0.00015166666666666668,
      "loss": 0.8553,
      "step": 2090
    },
    {
      "epoch": 0.3169332930878358,
      "grad_norm": 2.3716626167297363,
      "learning_rate": 0.00015,
      "loss": 0.7123,
      "step": 2100
    },
    {
      "epoch": 0.3184424992453969,
      "grad_norm": 6.195247650146484,
      "learning_rate": 0.00014833333333333335,
      "loss": 0.8539,
      "step": 2110
    },
    {
      "epoch": 0.31995170540295803,
      "grad_norm": 1.2907744646072388,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.6738,
      "step": 2120
    },
    {
      "epoch": 0.32146091156051915,
      "grad_norm": 2.870208263397217,
      "learning_rate": 0.000145,
      "loss": 1.0047,
      "step": 2130
    },
    {
      "epoch": 0.32297011771808026,
      "grad_norm": 2.556776285171509,
      "learning_rate": 0.00014333333333333334,
      "loss": 0.6902,
      "step": 2140
    },
    {
      "epoch": 0.32447932387564143,
      "grad_norm": 2.4027750492095947,
      "learning_rate": 0.00014166666666666668,
      "loss": 0.9,
      "step": 2150
    },
    {
      "epoch": 0.32598853003320255,
      "grad_norm": 3.6299126148223877,
      "learning_rate": 0.00014000000000000001,
      "loss": 0.7054,
      "step": 2160
    },
    {
      "epoch": 0.32749773619076367,
      "grad_norm": 5.872488975524902,
      "learning_rate": 0.00013833333333333333,
      "loss": 0.9789,
      "step": 2170
    },
    {
      "epoch": 0.3290069423483248,
      "grad_norm": 2.4122984409332275,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.777,
      "step": 2180
    },
    {
      "epoch": 0.3305161485058859,
      "grad_norm": 1.5129585266113281,
      "learning_rate": 0.000135,
      "loss": 0.8262,
      "step": 2190
    },
    {
      "epoch": 0.332025354663447,
      "grad_norm": 5.007599353790283,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.7295,
      "step": 2200
    },
    {
      "epoch": 0.33353456082100813,
      "grad_norm": 4.069816589355469,
      "learning_rate": 0.00013166666666666665,
      "loss": 0.7329,
      "step": 2210
    },
    {
      "epoch": 0.33504376697856925,
      "grad_norm": 2.215474843978882,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.6965,
      "step": 2220
    },
    {
      "epoch": 0.3365529731361304,
      "grad_norm": 4.424084186553955,
      "learning_rate": 0.00012833333333333333,
      "loss": 0.8938,
      "step": 2230
    },
    {
      "epoch": 0.33806217929369153,
      "grad_norm": 2.083667755126953,
      "learning_rate": 0.0001266666666666667,
      "loss": 0.6797,
      "step": 2240
    },
    {
      "epoch": 0.33957138545125265,
      "grad_norm": 3.270616054534912,
      "learning_rate": 0.000125,
      "loss": 0.9846,
      "step": 2250
    },
    {
      "epoch": 0.34108059160881377,
      "grad_norm": 2.3719124794006348,
      "learning_rate": 0.00012333333333333334,
      "loss": 0.9889,
      "step": 2260
    },
    {
      "epoch": 0.3425897977663749,
      "grad_norm": 4.526899814605713,
      "learning_rate": 0.00012166666666666668,
      "loss": 0.7161,
      "step": 2270
    },
    {
      "epoch": 0.344099003923936,
      "grad_norm": 2.448317289352417,
      "learning_rate": 0.00012,
      "loss": 1.0576,
      "step": 2280
    },
    {
      "epoch": 0.3456082100814971,
      "grad_norm": 6.386077404022217,
      "learning_rate": 0.00011833333333333334,
      "loss": 0.7523,
      "step": 2290
    },
    {
      "epoch": 0.34711741623905823,
      "grad_norm": 1.5083203315734863,
      "learning_rate": 0.00011666666666666667,
      "loss": 0.6011,
      "step": 2300
    },
    {
      "epoch": 0.3486266223966194,
      "grad_norm": 3.224774122238159,
      "learning_rate": 0.000115,
      "loss": 0.7648,
      "step": 2310
    },
    {
      "epoch": 0.3501358285541805,
      "grad_norm": 2.601732015609741,
      "learning_rate": 0.00011333333333333333,
      "loss": 0.5482,
      "step": 2320
    },
    {
      "epoch": 0.35164503471174163,
      "grad_norm": 2.8853044509887695,
      "learning_rate": 0.00011166666666666667,
      "loss": 0.7848,
      "step": 2330
    },
    {
      "epoch": 0.35315424086930275,
      "grad_norm": 2.837477922439575,
      "learning_rate": 0.00011,
      "loss": 0.7485,
      "step": 2340
    },
    {
      "epoch": 0.35466344702686387,
      "grad_norm": 2.3725178241729736,
      "learning_rate": 0.00010833333333333334,
      "loss": 0.6917,
      "step": 2350
    },
    {
      "epoch": 0.356172653184425,
      "grad_norm": 3.3595452308654785,
      "learning_rate": 0.00010666666666666668,
      "loss": 0.9133,
      "step": 2360
    },
    {
      "epoch": 0.3576818593419861,
      "grad_norm": 2.4229884147644043,
      "learning_rate": 0.000105,
      "loss": 0.8633,
      "step": 2370
    },
    {
      "epoch": 0.3591910654995472,
      "grad_norm": 14.06248950958252,
      "learning_rate": 0.00010333333333333333,
      "loss": 0.7344,
      "step": 2380
    },
    {
      "epoch": 0.3607002716571084,
      "grad_norm": 5.438836097717285,
      "learning_rate": 0.00010166666666666667,
      "loss": 0.8546,
      "step": 2390
    },
    {
      "epoch": 0.3622094778146695,
      "grad_norm": 1.7923352718353271,
      "learning_rate": 0.0001,
      "loss": 0.6723,
      "step": 2400
    },
    {
      "epoch": 0.3637186839722306,
      "grad_norm": 2.393676280975342,
      "learning_rate": 9.833333333333333e-05,
      "loss": 0.9543,
      "step": 2410
    },
    {
      "epoch": 0.36522789012979173,
      "grad_norm": 1.9984937906265259,
      "learning_rate": 9.666666666666667e-05,
      "loss": 1.008,
      "step": 2420
    },
    {
      "epoch": 0.36673709628735285,
      "grad_norm": 4.723394870758057,
      "learning_rate": 9.5e-05,
      "loss": 0.8371,
      "step": 2430
    },
    {
      "epoch": 0.36824630244491396,
      "grad_norm": 3.4928784370422363,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.6996,
      "step": 2440
    },
    {
      "epoch": 0.3697555086024751,
      "grad_norm": 7.16400671005249,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.7629,
      "step": 2450
    },
    {
      "epoch": 0.3712647147600362,
      "grad_norm": 4.33622407913208,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.8379,
      "step": 2460
    },
    {
      "epoch": 0.37277392091759737,
      "grad_norm": 6.066431522369385,
      "learning_rate": 8.833333333333333e-05,
      "loss": 0.7723,
      "step": 2470
    },
    {
      "epoch": 0.3742831270751585,
      "grad_norm": 3.8237087726593018,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.6625,
      "step": 2480
    },
    {
      "epoch": 0.3757923332327196,
      "grad_norm": 2.583082675933838,
      "learning_rate": 8.5e-05,
      "loss": 0.948,
      "step": 2490
    },
    {
      "epoch": 0.3773015393902807,
      "grad_norm": 2.209866523742676,
      "learning_rate": 8.333333333333333e-05,
      "loss": 0.677,
      "step": 2500
    },
    {
      "epoch": 0.3773015393902807,
      "eval_bleu-4": 0.04639638501635575,
      "eval_rouge-1": 27.79597178043631,
      "eval_rouge-2": 26.838494581280788,
      "eval_rouge-l": 8.241552146375792,
      "eval_runtime": 408.8721,
      "eval_samples_per_second": 3.475,
      "eval_steps_per_second": 0.871,
      "step": 2500
    },
    {
      "epoch": 0.37881074554784183,
      "grad_norm": 2.32407283782959,
      "learning_rate": 8.166666666666667e-05,
      "loss": 0.9348,
      "step": 2510
    },
    {
      "epoch": 0.38031995170540295,
      "grad_norm": 2.0966944694519043,
      "learning_rate": 8e-05,
      "loss": 0.9723,
      "step": 2520
    },
    {
      "epoch": 0.38182915786296406,
      "grad_norm": 1.9399375915527344,
      "learning_rate": 7.833333333333334e-05,
      "loss": 0.733,
      "step": 2530
    },
    {
      "epoch": 0.3833383640205252,
      "grad_norm": 1.5168720483779907,
      "learning_rate": 7.666666666666667e-05,
      "loss": 0.6254,
      "step": 2540
    },
    {
      "epoch": 0.38484757017808635,
      "grad_norm": 6.851475715637207,
      "learning_rate": 7.5e-05,
      "loss": 0.7693,
      "step": 2550
    },
    {
      "epoch": 0.38635677633564747,
      "grad_norm": 1.5773130655288696,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.7305,
      "step": 2560
    },
    {
      "epoch": 0.3878659824932086,
      "grad_norm": 3.597158432006836,
      "learning_rate": 7.166666666666667e-05,
      "loss": 0.665,
      "step": 2570
    },
    {
      "epoch": 0.3893751886507697,
      "grad_norm": 2.666429042816162,
      "learning_rate": 7.000000000000001e-05,
      "loss": 0.8127,
      "step": 2580
    },
    {
      "epoch": 0.3908843948083308,
      "grad_norm": 4.183495044708252,
      "learning_rate": 6.833333333333333e-05,
      "loss": 0.9168,
      "step": 2590
    },
    {
      "epoch": 0.39239360096589193,
      "grad_norm": 4.066106796264648,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.6885,
      "step": 2600
    },
    {
      "epoch": 0.39390280712345305,
      "grad_norm": 6.922797679901123,
      "learning_rate": 6.500000000000001e-05,
      "loss": 0.8402,
      "step": 2610
    },
    {
      "epoch": 0.39541201328101416,
      "grad_norm": 2.446794033050537,
      "learning_rate": 6.333333333333335e-05,
      "loss": 0.7652,
      "step": 2620
    },
    {
      "epoch": 0.39692121943857533,
      "grad_norm": 2.002314329147339,
      "learning_rate": 6.166666666666667e-05,
      "loss": 0.7017,
      "step": 2630
    },
    {
      "epoch": 0.39843042559613645,
      "grad_norm": 2.621840476989746,
      "learning_rate": 6e-05,
      "loss": 0.8023,
      "step": 2640
    },
    {
      "epoch": 0.39993963175369757,
      "grad_norm": 3.3448054790496826,
      "learning_rate": 5.833333333333333e-05,
      "loss": 0.7838,
      "step": 2650
    },
    {
      "epoch": 0.4014488379112587,
      "grad_norm": 6.148334503173828,
      "learning_rate": 5.6666666666666664e-05,
      "loss": 0.7824,
      "step": 2660
    },
    {
      "epoch": 0.4029580440688198,
      "grad_norm": 7.286426067352295,
      "learning_rate": 5.5e-05,
      "loss": 0.8752,
      "step": 2670
    },
    {
      "epoch": 0.4044672502263809,
      "grad_norm": 3.3784775733947754,
      "learning_rate": 5.333333333333334e-05,
      "loss": 0.7973,
      "step": 2680
    },
    {
      "epoch": 0.40597645638394203,
      "grad_norm": 2.51495361328125,
      "learning_rate": 5.1666666666666664e-05,
      "loss": 0.5734,
      "step": 2690
    },
    {
      "epoch": 0.40748566254150315,
      "grad_norm": 7.1158342361450195,
      "learning_rate": 5e-05,
      "loss": 0.7637,
      "step": 2700
    },
    {
      "epoch": 0.4089948686990643,
      "grad_norm": 2.5345561504364014,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.8307,
      "step": 2710
    },
    {
      "epoch": 0.41050407485662543,
      "grad_norm": 3.9672482013702393,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.7431,
      "step": 2720
    },
    {
      "epoch": 0.41201328101418655,
      "grad_norm": 2.597874164581299,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 0.715,
      "step": 2730
    },
    {
      "epoch": 0.41352248717174767,
      "grad_norm": 4.159081935882568,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.792,
      "step": 2740
    },
    {
      "epoch": 0.4150316933293088,
      "grad_norm": 2.3523776531219482,
      "learning_rate": 4.1666666666666665e-05,
      "loss": 0.717,
      "step": 2750
    },
    {
      "epoch": 0.4165408994868699,
      "grad_norm": 3.3046886920928955,
      "learning_rate": 4e-05,
      "loss": 0.5757,
      "step": 2760
    },
    {
      "epoch": 0.418050105644431,
      "grad_norm": 3.085684061050415,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.9211,
      "step": 2770
    },
    {
      "epoch": 0.41955931180199213,
      "grad_norm": 2.5912399291992188,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.715,
      "step": 2780
    },
    {
      "epoch": 0.4210685179595533,
      "grad_norm": 3.058556079864502,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 0.7732,
      "step": 2790
    },
    {
      "epoch": 0.4225777241171144,
      "grad_norm": 3.6382200717926025,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.5916,
      "step": 2800
    },
    {
      "epoch": 0.42408693027467553,
      "grad_norm": 1.6876493692398071,
      "learning_rate": 3.166666666666667e-05,
      "loss": 0.7064,
      "step": 2810
    },
    {
      "epoch": 0.42559613643223665,
      "grad_norm": 3.0838468074798584,
      "learning_rate": 3e-05,
      "loss": 0.6387,
      "step": 2820
    },
    {
      "epoch": 0.42710534258979777,
      "grad_norm": 3.6570281982421875,
      "learning_rate": 2.8333333333333332e-05,
      "loss": 0.7783,
      "step": 2830
    },
    {
      "epoch": 0.4286145487473589,
      "grad_norm": 1.608297348022461,
      "learning_rate": 2.666666666666667e-05,
      "loss": 0.933,
      "step": 2840
    },
    {
      "epoch": 0.43012375490492,
      "grad_norm": 3.340278148651123,
      "learning_rate": 2.5e-05,
      "loss": 0.7342,
      "step": 2850
    },
    {
      "epoch": 0.4316329610624811,
      "grad_norm": 4.855523586273193,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.8244,
      "step": 2860
    },
    {
      "epoch": 0.4331421672200423,
      "grad_norm": 4.077301025390625,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.616,
      "step": 2870
    },
    {
      "epoch": 0.4346513733776034,
      "grad_norm": 3.399177074432373,
      "learning_rate": 2e-05,
      "loss": 0.8031,
      "step": 2880
    },
    {
      "epoch": 0.4361605795351645,
      "grad_norm": 2.897024154663086,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.6002,
      "step": 2890
    },
    {
      "epoch": 0.43766978569272563,
      "grad_norm": 3.7338311672210693,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.5662,
      "step": 2900
    },
    {
      "epoch": 0.43917899185028675,
      "grad_norm": 2.24050235748291,
      "learning_rate": 1.5e-05,
      "loss": 0.7973,
      "step": 2910
    },
    {
      "epoch": 0.44068819800784786,
      "grad_norm": 2.962820529937744,
      "learning_rate": 1.3333333333333335e-05,
      "loss": 0.8146,
      "step": 2920
    },
    {
      "epoch": 0.442197404165409,
      "grad_norm": 2.974372625350952,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.8348,
      "step": 2930
    },
    {
      "epoch": 0.4437066103229701,
      "grad_norm": 2.966320037841797,
      "learning_rate": 1e-05,
      "loss": 0.9244,
      "step": 2940
    },
    {
      "epoch": 0.44521581648053127,
      "grad_norm": 6.462397575378418,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.7434,
      "step": 2950
    },
    {
      "epoch": 0.4467250226380924,
      "grad_norm": 2.2156755924224854,
      "learning_rate": 6.6666666666666675e-06,
      "loss": 0.832,
      "step": 2960
    },
    {
      "epoch": 0.4482342287956535,
      "grad_norm": 3.297116279602051,
      "learning_rate": 5e-06,
      "loss": 0.8031,
      "step": 2970
    },
    {
      "epoch": 0.4497434349532146,
      "grad_norm": 2.7277095317840576,
      "learning_rate": 3.3333333333333337e-06,
      "loss": 0.6842,
      "step": 2980
    },
    {
      "epoch": 0.45125264111077573,
      "grad_norm": 2.3754396438598633,
      "learning_rate": 1.6666666666666669e-06,
      "loss": 0.7002,
      "step": 2990
    },
    {
      "epoch": 0.45276184726833685,
      "grad_norm": 3.0457677841186523,
      "learning_rate": 0.0,
      "loss": 0.6266,
      "step": 3000
    },
    {
      "epoch": 0.45276184726833685,
      "eval_bleu-4": 0.04652875136732739,
      "eval_rouge-1": 27.358898170302602,
      "eval_rouge-2": 26.720923645320195,
      "eval_rouge-l": 8.265794581280788,
      "eval_runtime": 442.77,
      "eval_samples_per_second": 3.209,
      "eval_steps_per_second": 0.804,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.3216673699987456e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
