{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6122448979591837,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0020408163265306124,
      "grad_norm": 4.440054893493652,
      "learning_rate": 0.0004983333333333334,
      "loss": 2.1801,
      "step": 10
    },
    {
      "epoch": 0.004081632653061225,
      "grad_norm": 4.074328422546387,
      "learning_rate": 0.0004966666666666666,
      "loss": 1.3564,
      "step": 20
    },
    {
      "epoch": 0.006122448979591836,
      "grad_norm": 2.917604684829712,
      "learning_rate": 0.000495,
      "loss": 1.2643,
      "step": 30
    },
    {
      "epoch": 0.00816326530612245,
      "grad_norm": 4.249398231506348,
      "learning_rate": 0.0004933333333333334,
      "loss": 1.0824,
      "step": 40
    },
    {
      "epoch": 0.01020408163265306,
      "grad_norm": 6.799854755401611,
      "learning_rate": 0.0004916666666666666,
      "loss": 1.3695,
      "step": 50
    },
    {
      "epoch": 0.012244897959183673,
      "grad_norm": 2.2874040603637695,
      "learning_rate": 0.00049,
      "loss": 1.259,
      "step": 60
    },
    {
      "epoch": 0.014285714285714285,
      "grad_norm": 5.564243793487549,
      "learning_rate": 0.0004883333333333333,
      "loss": 1.2943,
      "step": 70
    },
    {
      "epoch": 0.0163265306122449,
      "grad_norm": 7.03342866897583,
      "learning_rate": 0.0004866666666666667,
      "loss": 1.1229,
      "step": 80
    },
    {
      "epoch": 0.018367346938775512,
      "grad_norm": 2.951277494430542,
      "learning_rate": 0.00048499999999999997,
      "loss": 1.078,
      "step": 90
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 9.011185646057129,
      "learning_rate": 0.00048333333333333334,
      "loss": 1.202,
      "step": 100
    },
    {
      "epoch": 0.022448979591836733,
      "grad_norm": 4.291881561279297,
      "learning_rate": 0.0004816666666666667,
      "loss": 1.2563,
      "step": 110
    },
    {
      "epoch": 0.024489795918367346,
      "grad_norm": 7.137035846710205,
      "learning_rate": 0.00048,
      "loss": 1.3262,
      "step": 120
    },
    {
      "epoch": 0.026530612244897958,
      "grad_norm": 3.3668487071990967,
      "learning_rate": 0.0004783333333333333,
      "loss": 0.9674,
      "step": 130
    },
    {
      "epoch": 0.02857142857142857,
      "grad_norm": 3.791053056716919,
      "learning_rate": 0.0004766666666666667,
      "loss": 1.0777,
      "step": 140
    },
    {
      "epoch": 0.030612244897959183,
      "grad_norm": 2.436770439147949,
      "learning_rate": 0.000475,
      "loss": 1.0582,
      "step": 150
    },
    {
      "epoch": 0.0326530612244898,
      "grad_norm": 2.580324172973633,
      "learning_rate": 0.00047333333333333336,
      "loss": 0.7053,
      "step": 160
    },
    {
      "epoch": 0.03469387755102041,
      "grad_norm": 2.1271607875823975,
      "learning_rate": 0.0004716666666666667,
      "loss": 0.8355,
      "step": 170
    },
    {
      "epoch": 0.036734693877551024,
      "grad_norm": 5.23621129989624,
      "learning_rate": 0.00047,
      "loss": 1.4414,
      "step": 180
    },
    {
      "epoch": 0.03877551020408163,
      "grad_norm": 6.05755615234375,
      "learning_rate": 0.00046833333333333335,
      "loss": 1.477,
      "step": 190
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 2.6887617111206055,
      "learning_rate": 0.00046666666666666666,
      "loss": 0.9943,
      "step": 200
    },
    {
      "epoch": 0.04285714285714286,
      "grad_norm": 4.35934591293335,
      "learning_rate": 0.000465,
      "loss": 1.227,
      "step": 210
    },
    {
      "epoch": 0.044897959183673466,
      "grad_norm": 2.6504533290863037,
      "learning_rate": 0.00046333333333333334,
      "loss": 0.9082,
      "step": 220
    },
    {
      "epoch": 0.04693877551020408,
      "grad_norm": 2.4173195362091064,
      "learning_rate": 0.0004616666666666667,
      "loss": 0.9801,
      "step": 230
    },
    {
      "epoch": 0.04897959183673469,
      "grad_norm": 1.8462556600570679,
      "learning_rate": 0.00046,
      "loss": 0.9473,
      "step": 240
    },
    {
      "epoch": 0.05102040816326531,
      "grad_norm": 5.717527389526367,
      "learning_rate": 0.0004583333333333333,
      "loss": 1.2447,
      "step": 250
    },
    {
      "epoch": 0.053061224489795916,
      "grad_norm": 6.471555709838867,
      "learning_rate": 0.0004566666666666667,
      "loss": 1.3531,
      "step": 260
    },
    {
      "epoch": 0.05510204081632653,
      "grad_norm": 1.9047657251358032,
      "learning_rate": 0.000455,
      "loss": 0.8205,
      "step": 270
    },
    {
      "epoch": 0.05714285714285714,
      "grad_norm": 7.16540002822876,
      "learning_rate": 0.0004533333333333333,
      "loss": 1.0283,
      "step": 280
    },
    {
      "epoch": 0.05918367346938776,
      "grad_norm": 5.780049800872803,
      "learning_rate": 0.0004516666666666667,
      "loss": 1.1539,
      "step": 290
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 3.34371018409729,
      "learning_rate": 0.00045000000000000004,
      "loss": 1.0799,
      "step": 300
    },
    {
      "epoch": 0.06326530612244897,
      "grad_norm": 3.589611053466797,
      "learning_rate": 0.0004483333333333333,
      "loss": 1.1455,
      "step": 310
    },
    {
      "epoch": 0.0653061224489796,
      "grad_norm": 18.078428268432617,
      "learning_rate": 0.00044666666666666666,
      "loss": 1.2383,
      "step": 320
    },
    {
      "epoch": 0.0673469387755102,
      "grad_norm": 6.515029430389404,
      "learning_rate": 0.00044500000000000003,
      "loss": 1.1621,
      "step": 330
    },
    {
      "epoch": 0.06938775510204082,
      "grad_norm": 7.713688850402832,
      "learning_rate": 0.00044333333333333334,
      "loss": 1.2947,
      "step": 340
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 4.644087791442871,
      "learning_rate": 0.00044166666666666665,
      "loss": 0.9432,
      "step": 350
    },
    {
      "epoch": 0.07346938775510205,
      "grad_norm": 4.593342304229736,
      "learning_rate": 0.00044,
      "loss": 1.0053,
      "step": 360
    },
    {
      "epoch": 0.07551020408163266,
      "grad_norm": 2.6065564155578613,
      "learning_rate": 0.0004383333333333334,
      "loss": 1.0455,
      "step": 370
    },
    {
      "epoch": 0.07755102040816327,
      "grad_norm": 3.8743252754211426,
      "learning_rate": 0.00043666666666666664,
      "loss": 1.0996,
      "step": 380
    },
    {
      "epoch": 0.07959183673469387,
      "grad_norm": 4.232469081878662,
      "learning_rate": 0.000435,
      "loss": 1.0973,
      "step": 390
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 5.078418731689453,
      "learning_rate": 0.00043333333333333337,
      "loss": 1.2328,
      "step": 400
    },
    {
      "epoch": 0.0836734693877551,
      "grad_norm": 3.503283739089966,
      "learning_rate": 0.0004316666666666667,
      "loss": 0.9949,
      "step": 410
    },
    {
      "epoch": 0.08571428571428572,
      "grad_norm": 3.164165496826172,
      "learning_rate": 0.00043,
      "loss": 1.2617,
      "step": 420
    },
    {
      "epoch": 0.08775510204081632,
      "grad_norm": 3.41935133934021,
      "learning_rate": 0.00042833333333333335,
      "loss": 1.0422,
      "step": 430
    },
    {
      "epoch": 0.08979591836734693,
      "grad_norm": 4.322776794433594,
      "learning_rate": 0.0004266666666666667,
      "loss": 1.0574,
      "step": 440
    },
    {
      "epoch": 0.09183673469387756,
      "grad_norm": 5.288330554962158,
      "learning_rate": 0.000425,
      "loss": 0.8271,
      "step": 450
    },
    {
      "epoch": 0.09387755102040816,
      "grad_norm": 2.0823402404785156,
      "learning_rate": 0.00042333333333333334,
      "loss": 0.9719,
      "step": 460
    },
    {
      "epoch": 0.09591836734693877,
      "grad_norm": 8.943230628967285,
      "learning_rate": 0.0004216666666666667,
      "loss": 0.8562,
      "step": 470
    },
    {
      "epoch": 0.09795918367346938,
      "grad_norm": 5.132660865783691,
      "learning_rate": 0.00042,
      "loss": 0.9006,
      "step": 480
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.858168601989746,
      "learning_rate": 0.00041833333333333333,
      "loss": 1.0793,
      "step": 490
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 8.152978897094727,
      "learning_rate": 0.0004166666666666667,
      "loss": 0.9762,
      "step": 500
    },
    {
      "epoch": 0.10204081632653061,
      "eval_bleu-4": 0.043858552472448215,
      "eval_rouge-1": 32.52665390476191,
      "eval_rouge-2": 29.618903523809525,
      "eval_rouge-l": 7.834705142857143,
      "eval_runtime": 507.7438,
      "eval_samples_per_second": 2.068,
      "eval_steps_per_second": 0.518,
      "step": 500
    },
    {
      "epoch": 0.10408163265306122,
      "grad_norm": 5.739871978759766,
      "learning_rate": 0.000415,
      "loss": 1.202,
      "step": 510
    },
    {
      "epoch": 0.10612244897959183,
      "grad_norm": 7.187209606170654,
      "learning_rate": 0.0004133333333333333,
      "loss": 0.8273,
      "step": 520
    },
    {
      "epoch": 0.10816326530612246,
      "grad_norm": 5.227106094360352,
      "learning_rate": 0.0004116666666666667,
      "loss": 1.0625,
      "step": 530
    },
    {
      "epoch": 0.11020408163265306,
      "grad_norm": 5.042181491851807,
      "learning_rate": 0.00041,
      "loss": 1.0621,
      "step": 540
    },
    {
      "epoch": 0.11224489795918367,
      "grad_norm": 4.002438545227051,
      "learning_rate": 0.00040833333333333336,
      "loss": 1.257,
      "step": 550
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 4.46075963973999,
      "learning_rate": 0.00040666666666666667,
      "loss": 1.0461,
      "step": 560
    },
    {
      "epoch": 0.11632653061224489,
      "grad_norm": 7.494292259216309,
      "learning_rate": 0.00040500000000000003,
      "loss": 0.9572,
      "step": 570
    },
    {
      "epoch": 0.11836734693877551,
      "grad_norm": 7.415000915527344,
      "learning_rate": 0.00040333333333333334,
      "loss": 0.9475,
      "step": 580
    },
    {
      "epoch": 0.12040816326530612,
      "grad_norm": 3.966010332107544,
      "learning_rate": 0.00040166666666666665,
      "loss": 1.1031,
      "step": 590
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 4.81483793258667,
      "learning_rate": 0.0004,
      "loss": 0.8539,
      "step": 600
    },
    {
      "epoch": 0.12448979591836734,
      "grad_norm": 4.582952499389648,
      "learning_rate": 0.00039833333333333333,
      "loss": 0.9285,
      "step": 610
    },
    {
      "epoch": 0.12653061224489795,
      "grad_norm": 3.466120481491089,
      "learning_rate": 0.0003966666666666667,
      "loss": 1.007,
      "step": 620
    },
    {
      "epoch": 0.12857142857142856,
      "grad_norm": 4.112790584564209,
      "learning_rate": 0.000395,
      "loss": 0.9527,
      "step": 630
    },
    {
      "epoch": 0.1306122448979592,
      "grad_norm": 7.0106353759765625,
      "learning_rate": 0.0003933333333333333,
      "loss": 1.075,
      "step": 640
    },
    {
      "epoch": 0.1326530612244898,
      "grad_norm": 3.2250001430511475,
      "learning_rate": 0.0003916666666666667,
      "loss": 0.8217,
      "step": 650
    },
    {
      "epoch": 0.1346938775510204,
      "grad_norm": 6.137421131134033,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.9235,
      "step": 660
    },
    {
      "epoch": 0.13673469387755102,
      "grad_norm": 6.2576751708984375,
      "learning_rate": 0.0003883333333333333,
      "loss": 1.3125,
      "step": 670
    },
    {
      "epoch": 0.13877551020408163,
      "grad_norm": 4.5324273109436035,
      "learning_rate": 0.00038666666666666667,
      "loss": 1.0264,
      "step": 680
    },
    {
      "epoch": 0.14081632653061224,
      "grad_norm": 6.986923694610596,
      "learning_rate": 0.00038500000000000003,
      "loss": 0.7968,
      "step": 690
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 6.207892894744873,
      "learning_rate": 0.00038333333333333334,
      "loss": 1.1809,
      "step": 700
    },
    {
      "epoch": 0.14489795918367346,
      "grad_norm": 5.026271820068359,
      "learning_rate": 0.00038166666666666666,
      "loss": 1.2736,
      "step": 710
    },
    {
      "epoch": 0.1469387755102041,
      "grad_norm": 4.06048583984375,
      "learning_rate": 0.00038,
      "loss": 1.2105,
      "step": 720
    },
    {
      "epoch": 0.1489795918367347,
      "grad_norm": 3.1573009490966797,
      "learning_rate": 0.0003783333333333334,
      "loss": 0.8996,
      "step": 730
    },
    {
      "epoch": 0.1510204081632653,
      "grad_norm": 5.482696533203125,
      "learning_rate": 0.00037666666666666664,
      "loss": 1.0137,
      "step": 740
    },
    {
      "epoch": 0.15306122448979592,
      "grad_norm": 5.8262481689453125,
      "learning_rate": 0.000375,
      "loss": 0.7754,
      "step": 750
    },
    {
      "epoch": 0.15510204081632653,
      "grad_norm": 7.824552536010742,
      "learning_rate": 0.0003733333333333334,
      "loss": 1.1434,
      "step": 760
    },
    {
      "epoch": 0.15714285714285714,
      "grad_norm": 5.245066165924072,
      "learning_rate": 0.00037166666666666663,
      "loss": 0.8254,
      "step": 770
    },
    {
      "epoch": 0.15918367346938775,
      "grad_norm": 2.3860015869140625,
      "learning_rate": 0.00037,
      "loss": 0.9697,
      "step": 780
    },
    {
      "epoch": 0.16122448979591836,
      "grad_norm": 3.9309475421905518,
      "learning_rate": 0.00036833333333333336,
      "loss": 0.9393,
      "step": 790
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 11.014583587646484,
      "learning_rate": 0.00036666666666666667,
      "loss": 1.0465,
      "step": 800
    },
    {
      "epoch": 0.1653061224489796,
      "grad_norm": 8.746559143066406,
      "learning_rate": 0.000365,
      "loss": 0.9785,
      "step": 810
    },
    {
      "epoch": 0.1673469387755102,
      "grad_norm": 4.564396858215332,
      "learning_rate": 0.00036333333333333335,
      "loss": 0.9066,
      "step": 820
    },
    {
      "epoch": 0.16938775510204082,
      "grad_norm": 4.376824855804443,
      "learning_rate": 0.0003616666666666667,
      "loss": 0.9617,
      "step": 830
    },
    {
      "epoch": 0.17142857142857143,
      "grad_norm": 6.721683979034424,
      "learning_rate": 0.00035999999999999997,
      "loss": 1.0176,
      "step": 840
    },
    {
      "epoch": 0.17346938775510204,
      "grad_norm": 5.849215030670166,
      "learning_rate": 0.00035833333333333333,
      "loss": 0.8457,
      "step": 850
    },
    {
      "epoch": 0.17551020408163265,
      "grad_norm": 6.799557685852051,
      "learning_rate": 0.0003566666666666667,
      "loss": 0.8246,
      "step": 860
    },
    {
      "epoch": 0.17755102040816326,
      "grad_norm": 3.3883540630340576,
      "learning_rate": 0.000355,
      "loss": 0.9451,
      "step": 870
    },
    {
      "epoch": 0.17959183673469387,
      "grad_norm": 5.404979228973389,
      "learning_rate": 0.0003533333333333333,
      "loss": 1.0525,
      "step": 880
    },
    {
      "epoch": 0.1816326530612245,
      "grad_norm": 5.5553059577941895,
      "learning_rate": 0.0003516666666666667,
      "loss": 0.9727,
      "step": 890
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 4.107810020446777,
      "learning_rate": 0.00035,
      "loss": 1.0557,
      "step": 900
    },
    {
      "epoch": 0.18571428571428572,
      "grad_norm": 4.133194446563721,
      "learning_rate": 0.00034833333333333336,
      "loss": 0.9566,
      "step": 910
    },
    {
      "epoch": 0.18775510204081633,
      "grad_norm": 4.891989707946777,
      "learning_rate": 0.00034666666666666667,
      "loss": 1.1363,
      "step": 920
    },
    {
      "epoch": 0.18979591836734694,
      "grad_norm": 3.4668455123901367,
      "learning_rate": 0.000345,
      "loss": 0.8326,
      "step": 930
    },
    {
      "epoch": 0.19183673469387755,
      "grad_norm": 2.770578145980835,
      "learning_rate": 0.00034333333333333335,
      "loss": 1.016,
      "step": 940
    },
    {
      "epoch": 0.19387755102040816,
      "grad_norm": 8.119815826416016,
      "learning_rate": 0.00034166666666666666,
      "loss": 0.9055,
      "step": 950
    },
    {
      "epoch": 0.19591836734693877,
      "grad_norm": 4.602546215057373,
      "learning_rate": 0.00034,
      "loss": 0.8539,
      "step": 960
    },
    {
      "epoch": 0.19795918367346937,
      "grad_norm": 5.957247257232666,
      "learning_rate": 0.00033833333333333334,
      "loss": 0.9066,
      "step": 970
    },
    {
      "epoch": 0.2,
      "grad_norm": 4.245564937591553,
      "learning_rate": 0.0003366666666666667,
      "loss": 0.8268,
      "step": 980
    },
    {
      "epoch": 0.20204081632653062,
      "grad_norm": 5.748650550842285,
      "learning_rate": 0.000335,
      "loss": 1.0555,
      "step": 990
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 3.214949369430542,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.8441,
      "step": 1000
    },
    {
      "epoch": 0.20408163265306123,
      "eval_bleu-4": 0.04275978284411991,
      "eval_rouge-1": 26.030863333333333,
      "eval_rouge-2": 25.89338923809524,
      "eval_rouge-l": 7.725098571428571,
      "eval_runtime": 350.7979,
      "eval_samples_per_second": 2.993,
      "eval_steps_per_second": 0.75,
      "step": 1000
    },
    {
      "epoch": 0.20612244897959184,
      "grad_norm": 4.448604583740234,
      "learning_rate": 0.0003316666666666667,
      "loss": 0.7305,
      "step": 1010
    },
    {
      "epoch": 0.20816326530612245,
      "grad_norm": 5.223604679107666,
      "learning_rate": 0.00033,
      "loss": 1.0332,
      "step": 1020
    },
    {
      "epoch": 0.21020408163265306,
      "grad_norm": 4.023464679718018,
      "learning_rate": 0.0003283333333333333,
      "loss": 1.0867,
      "step": 1030
    },
    {
      "epoch": 0.21224489795918366,
      "grad_norm": 4.046242713928223,
      "learning_rate": 0.0003266666666666667,
      "loss": 0.9617,
      "step": 1040
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 4.80375862121582,
      "learning_rate": 0.00032500000000000004,
      "loss": 0.924,
      "step": 1050
    },
    {
      "epoch": 0.2163265306122449,
      "grad_norm": 2.8722312450408936,
      "learning_rate": 0.0003233333333333333,
      "loss": 0.9676,
      "step": 1060
    },
    {
      "epoch": 0.21836734693877552,
      "grad_norm": 4.395813941955566,
      "learning_rate": 0.00032166666666666666,
      "loss": 1.0445,
      "step": 1070
    },
    {
      "epoch": 0.22040816326530613,
      "grad_norm": 3.748638153076172,
      "learning_rate": 0.00032,
      "loss": 0.9473,
      "step": 1080
    },
    {
      "epoch": 0.22244897959183674,
      "grad_norm": 7.902772903442383,
      "learning_rate": 0.00031833333333333334,
      "loss": 1.1465,
      "step": 1090
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 2.972614288330078,
      "learning_rate": 0.00031666666666666665,
      "loss": 0.8051,
      "step": 1100
    },
    {
      "epoch": 0.22653061224489796,
      "grad_norm": 5.201937675476074,
      "learning_rate": 0.000315,
      "loss": 1.0295,
      "step": 1110
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 3.151660442352295,
      "learning_rate": 0.0003133333333333334,
      "loss": 1.0791,
      "step": 1120
    },
    {
      "epoch": 0.23061224489795917,
      "grad_norm": 4.91055965423584,
      "learning_rate": 0.00031166666666666663,
      "loss": 0.8451,
      "step": 1130
    },
    {
      "epoch": 0.23265306122448978,
      "grad_norm": 3.2116858959198,
      "learning_rate": 0.00031,
      "loss": 0.8793,
      "step": 1140
    },
    {
      "epoch": 0.23469387755102042,
      "grad_norm": 5.50111198425293,
      "learning_rate": 0.00030833333333333337,
      "loss": 1.0271,
      "step": 1150
    },
    {
      "epoch": 0.23673469387755103,
      "grad_norm": 4.446529865264893,
      "learning_rate": 0.0003066666666666667,
      "loss": 1.0877,
      "step": 1160
    },
    {
      "epoch": 0.23877551020408164,
      "grad_norm": 3.519918918609619,
      "learning_rate": 0.000305,
      "loss": 0.81,
      "step": 1170
    },
    {
      "epoch": 0.24081632653061225,
      "grad_norm": 2.9799892902374268,
      "learning_rate": 0.00030333333333333335,
      "loss": 0.7494,
      "step": 1180
    },
    {
      "epoch": 0.24285714285714285,
      "grad_norm": 10.385651588439941,
      "learning_rate": 0.0003016666666666667,
      "loss": 1.0281,
      "step": 1190
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 4.769197940826416,
      "learning_rate": 0.0003,
      "loss": 0.9453,
      "step": 1200
    },
    {
      "epoch": 0.24693877551020407,
      "grad_norm": 3.7351064682006836,
      "learning_rate": 0.00029833333333333334,
      "loss": 0.7704,
      "step": 1210
    },
    {
      "epoch": 0.24897959183673468,
      "grad_norm": 4.6446356773376465,
      "learning_rate": 0.0002966666666666667,
      "loss": 0.7559,
      "step": 1220
    },
    {
      "epoch": 0.2510204081632653,
      "grad_norm": 4.206218719482422,
      "learning_rate": 0.000295,
      "loss": 0.9036,
      "step": 1230
    },
    {
      "epoch": 0.2530612244897959,
      "grad_norm": 4.9213080406188965,
      "learning_rate": 0.0002933333333333333,
      "loss": 0.683,
      "step": 1240
    },
    {
      "epoch": 0.25510204081632654,
      "grad_norm": 3.515564441680908,
      "learning_rate": 0.0002916666666666667,
      "loss": 0.8145,
      "step": 1250
    },
    {
      "epoch": 0.2571428571428571,
      "grad_norm": 7.896862030029297,
      "learning_rate": 0.00029,
      "loss": 1.1516,
      "step": 1260
    },
    {
      "epoch": 0.25918367346938775,
      "grad_norm": 8.0061616897583,
      "learning_rate": 0.0002883333333333333,
      "loss": 1.2719,
      "step": 1270
    },
    {
      "epoch": 0.2612244897959184,
      "grad_norm": 6.057493209838867,
      "learning_rate": 0.0002866666666666667,
      "loss": 1.1006,
      "step": 1280
    },
    {
      "epoch": 0.26326530612244897,
      "grad_norm": 2.186504364013672,
      "learning_rate": 0.000285,
      "loss": 0.9197,
      "step": 1290
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 1.5993911027908325,
      "learning_rate": 0.00028333333333333335,
      "loss": 0.7867,
      "step": 1300
    },
    {
      "epoch": 0.2673469387755102,
      "grad_norm": 8.941581726074219,
      "learning_rate": 0.00028166666666666666,
      "loss": 0.857,
      "step": 1310
    },
    {
      "epoch": 0.2693877551020408,
      "grad_norm": 3.2893764972686768,
      "learning_rate": 0.00028000000000000003,
      "loss": 0.7555,
      "step": 1320
    },
    {
      "epoch": 0.2714285714285714,
      "grad_norm": 6.223530292510986,
      "learning_rate": 0.00027833333333333334,
      "loss": 0.8277,
      "step": 1330
    },
    {
      "epoch": 0.27346938775510204,
      "grad_norm": 2.546579360961914,
      "learning_rate": 0.00027666666666666665,
      "loss": 0.7031,
      "step": 1340
    },
    {
      "epoch": 0.2755102040816326,
      "grad_norm": 4.424273490905762,
      "learning_rate": 0.000275,
      "loss": 0.7345,
      "step": 1350
    },
    {
      "epoch": 0.27755102040816326,
      "grad_norm": 7.5921196937561035,
      "learning_rate": 0.00027333333333333333,
      "loss": 0.8141,
      "step": 1360
    },
    {
      "epoch": 0.2795918367346939,
      "grad_norm": 4.5916924476623535,
      "learning_rate": 0.0002716666666666667,
      "loss": 1.0709,
      "step": 1370
    },
    {
      "epoch": 0.2816326530612245,
      "grad_norm": 1.9729262590408325,
      "learning_rate": 0.00027,
      "loss": 0.7386,
      "step": 1380
    },
    {
      "epoch": 0.2836734693877551,
      "grad_norm": 7.596859931945801,
      "learning_rate": 0.0002683333333333333,
      "loss": 0.6916,
      "step": 1390
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 5.171772003173828,
      "learning_rate": 0.0002666666666666667,
      "loss": 1.0076,
      "step": 1400
    },
    {
      "epoch": 0.28775510204081634,
      "grad_norm": 4.427727699279785,
      "learning_rate": 0.00026500000000000004,
      "loss": 0.8844,
      "step": 1410
    },
    {
      "epoch": 0.2897959183673469,
      "grad_norm": 7.343652725219727,
      "learning_rate": 0.0002633333333333333,
      "loss": 1.1258,
      "step": 1420
    },
    {
      "epoch": 0.29183673469387755,
      "grad_norm": 3.1564226150512695,
      "learning_rate": 0.00026166666666666667,
      "loss": 0.7939,
      "step": 1430
    },
    {
      "epoch": 0.2938775510204082,
      "grad_norm": 1.2534481287002563,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.8812,
      "step": 1440
    },
    {
      "epoch": 0.29591836734693877,
      "grad_norm": 4.284315586090088,
      "learning_rate": 0.00025833333333333334,
      "loss": 0.9295,
      "step": 1450
    },
    {
      "epoch": 0.2979591836734694,
      "grad_norm": 4.454397678375244,
      "learning_rate": 0.00025666666666666665,
      "loss": 0.7848,
      "step": 1460
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.855536699295044,
      "learning_rate": 0.000255,
      "loss": 0.6403,
      "step": 1470
    },
    {
      "epoch": 0.3020408163265306,
      "grad_norm": 7.999338150024414,
      "learning_rate": 0.0002533333333333334,
      "loss": 0.8898,
      "step": 1480
    },
    {
      "epoch": 0.3040816326530612,
      "grad_norm": 2.391148328781128,
      "learning_rate": 0.00025166666666666664,
      "loss": 0.6346,
      "step": 1490
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 1.5155754089355469,
      "learning_rate": 0.00025,
      "loss": 0.8784,
      "step": 1500
    },
    {
      "epoch": 0.30612244897959184,
      "eval_bleu-4": 0.04293555197158082,
      "eval_rouge-1": 28.544734476190477,
      "eval_rouge-2": 26.953418095238096,
      "eval_rouge-l": 7.724506285714286,
      "eval_runtime": 366.6316,
      "eval_samples_per_second": 2.864,
      "eval_steps_per_second": 0.717,
      "step": 1500
    },
    {
      "epoch": 0.3081632653061224,
      "grad_norm": 3.8679566383361816,
      "learning_rate": 0.0002483333333333333,
      "loss": 0.8556,
      "step": 1510
    },
    {
      "epoch": 0.31020408163265306,
      "grad_norm": 5.467067241668701,
      "learning_rate": 0.0002466666666666667,
      "loss": 1.1924,
      "step": 1520
    },
    {
      "epoch": 0.3122448979591837,
      "grad_norm": 4.43603515625,
      "learning_rate": 0.000245,
      "loss": 0.6482,
      "step": 1530
    },
    {
      "epoch": 0.3142857142857143,
      "grad_norm": 5.475569248199463,
      "learning_rate": 0.00024333333333333336,
      "loss": 0.8641,
      "step": 1540
    },
    {
      "epoch": 0.3163265306122449,
      "grad_norm": 4.0920820236206055,
      "learning_rate": 0.00024166666666666667,
      "loss": 0.6911,
      "step": 1550
    },
    {
      "epoch": 0.3183673469387755,
      "grad_norm": 9.12923812866211,
      "learning_rate": 0.00024,
      "loss": 0.8787,
      "step": 1560
    },
    {
      "epoch": 0.32040816326530613,
      "grad_norm": 4.784415245056152,
      "learning_rate": 0.00023833333333333334,
      "loss": 0.9824,
      "step": 1570
    },
    {
      "epoch": 0.3224489795918367,
      "grad_norm": 6.138486862182617,
      "learning_rate": 0.00023666666666666668,
      "loss": 0.7623,
      "step": 1580
    },
    {
      "epoch": 0.32448979591836735,
      "grad_norm": 7.163136005401611,
      "learning_rate": 0.000235,
      "loss": 0.885,
      "step": 1590
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 6.169278144836426,
      "learning_rate": 0.00023333333333333333,
      "loss": 0.9938,
      "step": 1600
    },
    {
      "epoch": 0.32857142857142857,
      "grad_norm": 5.940835952758789,
      "learning_rate": 0.00023166666666666667,
      "loss": 0.7785,
      "step": 1610
    },
    {
      "epoch": 0.3306122448979592,
      "grad_norm": 5.04435920715332,
      "learning_rate": 0.00023,
      "loss": 0.8996,
      "step": 1620
    },
    {
      "epoch": 0.3326530612244898,
      "grad_norm": 3.906874656677246,
      "learning_rate": 0.00022833333333333334,
      "loss": 0.8129,
      "step": 1630
    },
    {
      "epoch": 0.3346938775510204,
      "grad_norm": 4.53237771987915,
      "learning_rate": 0.00022666666666666666,
      "loss": 0.6887,
      "step": 1640
    },
    {
      "epoch": 0.336734693877551,
      "grad_norm": 7.508608341217041,
      "learning_rate": 0.00022500000000000002,
      "loss": 0.8602,
      "step": 1650
    },
    {
      "epoch": 0.33877551020408164,
      "grad_norm": 3.0241615772247314,
      "learning_rate": 0.00022333333333333333,
      "loss": 0.6041,
      "step": 1660
    },
    {
      "epoch": 0.3408163265306122,
      "grad_norm": 3.435260772705078,
      "learning_rate": 0.00022166666666666667,
      "loss": 0.6654,
      "step": 1670
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 3.3263473510742188,
      "learning_rate": 0.00022,
      "loss": 0.9025,
      "step": 1680
    },
    {
      "epoch": 0.3448979591836735,
      "grad_norm": 6.360690116882324,
      "learning_rate": 0.00021833333333333332,
      "loss": 0.866,
      "step": 1690
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 2.861025333404541,
      "learning_rate": 0.00021666666666666668,
      "loss": 0.7734,
      "step": 1700
    },
    {
      "epoch": 0.3489795918367347,
      "grad_norm": 6.9013495445251465,
      "learning_rate": 0.000215,
      "loss": 0.7211,
      "step": 1710
    },
    {
      "epoch": 0.3510204081632653,
      "grad_norm": 6.733950138092041,
      "learning_rate": 0.00021333333333333336,
      "loss": 0.6744,
      "step": 1720
    },
    {
      "epoch": 0.35306122448979593,
      "grad_norm": 3.283778190612793,
      "learning_rate": 0.00021166666666666667,
      "loss": 0.7918,
      "step": 1730
    },
    {
      "epoch": 0.3551020408163265,
      "grad_norm": 4.60034704208374,
      "learning_rate": 0.00021,
      "loss": 0.7543,
      "step": 1740
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 3.7061872482299805,
      "learning_rate": 0.00020833333333333335,
      "loss": 0.7963,
      "step": 1750
    },
    {
      "epoch": 0.35918367346938773,
      "grad_norm": 8.798309326171875,
      "learning_rate": 0.00020666666666666666,
      "loss": 0.8471,
      "step": 1760
    },
    {
      "epoch": 0.36122448979591837,
      "grad_norm": 5.565502166748047,
      "learning_rate": 0.000205,
      "loss": 0.8293,
      "step": 1770
    },
    {
      "epoch": 0.363265306122449,
      "grad_norm": 7.930070877075195,
      "learning_rate": 0.00020333333333333333,
      "loss": 0.7129,
      "step": 1780
    },
    {
      "epoch": 0.3653061224489796,
      "grad_norm": 4.562704086303711,
      "learning_rate": 0.00020166666666666667,
      "loss": 0.9152,
      "step": 1790
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 4.427737236022949,
      "learning_rate": 0.0002,
      "loss": 0.9531,
      "step": 1800
    },
    {
      "epoch": 0.3693877551020408,
      "grad_norm": 3.009857654571533,
      "learning_rate": 0.00019833333333333335,
      "loss": 0.8242,
      "step": 1810
    },
    {
      "epoch": 0.37142857142857144,
      "grad_norm": 7.400479316711426,
      "learning_rate": 0.00019666666666666666,
      "loss": 0.807,
      "step": 1820
    },
    {
      "epoch": 0.373469387755102,
      "grad_norm": 5.3618621826171875,
      "learning_rate": 0.00019500000000000002,
      "loss": 1.0403,
      "step": 1830
    },
    {
      "epoch": 0.37551020408163266,
      "grad_norm": 3.5966861248016357,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.646,
      "step": 1840
    },
    {
      "epoch": 0.37755102040816324,
      "grad_norm": 3.6170177459716797,
      "learning_rate": 0.00019166666666666667,
      "loss": 0.8574,
      "step": 1850
    },
    {
      "epoch": 0.3795918367346939,
      "grad_norm": 4.459870338439941,
      "learning_rate": 0.00019,
      "loss": 0.7543,
      "step": 1860
    },
    {
      "epoch": 0.3816326530612245,
      "grad_norm": 4.6932244300842285,
      "learning_rate": 0.00018833333333333332,
      "loss": 0.982,
      "step": 1870
    },
    {
      "epoch": 0.3836734693877551,
      "grad_norm": 3.611302614212036,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.7863,
      "step": 1880
    },
    {
      "epoch": 0.38571428571428573,
      "grad_norm": 3.3103086948394775,
      "learning_rate": 0.000185,
      "loss": 0.832,
      "step": 1890
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 7.632986068725586,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.8629,
      "step": 1900
    },
    {
      "epoch": 0.38979591836734695,
      "grad_norm": 6.781040191650391,
      "learning_rate": 0.00018166666666666667,
      "loss": 0.6922,
      "step": 1910
    },
    {
      "epoch": 0.39183673469387753,
      "grad_norm": 3.4086709022521973,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.7932,
      "step": 1920
    },
    {
      "epoch": 0.39387755102040817,
      "grad_norm": 2.4932448863983154,
      "learning_rate": 0.00017833333333333335,
      "loss": 0.9158,
      "step": 1930
    },
    {
      "epoch": 0.39591836734693875,
      "grad_norm": 3.2252674102783203,
      "learning_rate": 0.00017666666666666666,
      "loss": 0.9227,
      "step": 1940
    },
    {
      "epoch": 0.3979591836734694,
      "grad_norm": 1.7995585203170776,
      "learning_rate": 0.000175,
      "loss": 0.7744,
      "step": 1950
    },
    {
      "epoch": 0.4,
      "grad_norm": 5.090501308441162,
      "learning_rate": 0.00017333333333333334,
      "loss": 1.0559,
      "step": 1960
    },
    {
      "epoch": 0.4020408163265306,
      "grad_norm": 3.564140796661377,
      "learning_rate": 0.00017166666666666667,
      "loss": 0.7412,
      "step": 1970
    },
    {
      "epoch": 0.40408163265306124,
      "grad_norm": 2.965848684310913,
      "learning_rate": 0.00017,
      "loss": 0.7328,
      "step": 1980
    },
    {
      "epoch": 0.4061224489795918,
      "grad_norm": 11.211166381835938,
      "learning_rate": 0.00016833333333333335,
      "loss": 0.8025,
      "step": 1990
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 3.780111074447632,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.7359,
      "step": 2000
    },
    {
      "epoch": 0.40816326530612246,
      "eval_bleu-4": 0.043122797441393156,
      "eval_rouge-1": 29.92509714285714,
      "eval_rouge-2": 26.952179428571426,
      "eval_rouge-l": 7.737135428571429,
      "eval_runtime": 451.7921,
      "eval_samples_per_second": 2.324,
      "eval_steps_per_second": 0.582,
      "step": 2000
    },
    {
      "epoch": 0.41020408163265304,
      "grad_norm": 3.871152400970459,
      "learning_rate": 0.000165,
      "loss": 0.8279,
      "step": 2010
    },
    {
      "epoch": 0.4122448979591837,
      "grad_norm": 3.2822024822235107,
      "learning_rate": 0.00016333333333333334,
      "loss": 0.5523,
      "step": 2020
    },
    {
      "epoch": 0.4142857142857143,
      "grad_norm": 3.01127028465271,
      "learning_rate": 0.00016166666666666665,
      "loss": 0.8924,
      "step": 2030
    },
    {
      "epoch": 0.4163265306122449,
      "grad_norm": 5.996100902557373,
      "learning_rate": 0.00016,
      "loss": 0.6037,
      "step": 2040
    },
    {
      "epoch": 0.41836734693877553,
      "grad_norm": 3.4603569507598877,
      "learning_rate": 0.00015833333333333332,
      "loss": 0.6172,
      "step": 2050
    },
    {
      "epoch": 0.4204081632653061,
      "grad_norm": 3.735044240951538,
      "learning_rate": 0.0001566666666666667,
      "loss": 0.7672,
      "step": 2060
    },
    {
      "epoch": 0.42244897959183675,
      "grad_norm": 4.955948829650879,
      "learning_rate": 0.000155,
      "loss": 0.8326,
      "step": 2070
    },
    {
      "epoch": 0.42448979591836733,
      "grad_norm": 1.5141167640686035,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.5555,
      "step": 2080
    },
    {
      "epoch": 0.42653061224489797,
      "grad_norm": 4.118048667907715,
      "learning_rate": 0.00015166666666666668,
      "loss": 0.7357,
      "step": 2090
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 2.6048641204833984,
      "learning_rate": 0.00015,
      "loss": 0.8949,
      "step": 2100
    },
    {
      "epoch": 0.4306122448979592,
      "grad_norm": 4.602220058441162,
      "learning_rate": 0.00014833333333333335,
      "loss": 0.9115,
      "step": 2110
    },
    {
      "epoch": 0.4326530612244898,
      "grad_norm": 10.279677391052246,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.6223,
      "step": 2120
    },
    {
      "epoch": 0.4346938775510204,
      "grad_norm": 2.7649385929107666,
      "learning_rate": 0.000145,
      "loss": 0.8895,
      "step": 2130
    },
    {
      "epoch": 0.43673469387755104,
      "grad_norm": 3.0087382793426514,
      "learning_rate": 0.00014333333333333334,
      "loss": 0.774,
      "step": 2140
    },
    {
      "epoch": 0.4387755102040816,
      "grad_norm": 4.783586502075195,
      "learning_rate": 0.00014166666666666668,
      "loss": 0.7973,
      "step": 2150
    },
    {
      "epoch": 0.44081632653061226,
      "grad_norm": 3.863581657409668,
      "learning_rate": 0.00014000000000000001,
      "loss": 0.8656,
      "step": 2160
    },
    {
      "epoch": 0.44285714285714284,
      "grad_norm": 3.8429670333862305,
      "learning_rate": 0.00013833333333333333,
      "loss": 0.9076,
      "step": 2170
    },
    {
      "epoch": 0.4448979591836735,
      "grad_norm": 7.764977931976318,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.6272,
      "step": 2180
    },
    {
      "epoch": 0.44693877551020406,
      "grad_norm": 3.0208420753479004,
      "learning_rate": 0.000135,
      "loss": 0.5963,
      "step": 2190
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 2.9164748191833496,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.7098,
      "step": 2200
    },
    {
      "epoch": 0.45102040816326533,
      "grad_norm": 5.613640308380127,
      "learning_rate": 0.00013166666666666665,
      "loss": 0.808,
      "step": 2210
    },
    {
      "epoch": 0.4530612244897959,
      "grad_norm": 5.91755485534668,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.7615,
      "step": 2220
    },
    {
      "epoch": 0.45510204081632655,
      "grad_norm": 2.7607269287109375,
      "learning_rate": 0.00012833333333333333,
      "loss": 0.8232,
      "step": 2230
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 3.666489839553833,
      "learning_rate": 0.0001266666666666667,
      "loss": 0.9328,
      "step": 2240
    },
    {
      "epoch": 0.45918367346938777,
      "grad_norm": 3.0774238109588623,
      "learning_rate": 0.000125,
      "loss": 0.5722,
      "step": 2250
    },
    {
      "epoch": 0.46122448979591835,
      "grad_norm": 6.037187576293945,
      "learning_rate": 0.00012333333333333334,
      "loss": 0.9682,
      "step": 2260
    },
    {
      "epoch": 0.463265306122449,
      "grad_norm": 3.365276575088501,
      "learning_rate": 0.00012166666666666668,
      "loss": 0.7256,
      "step": 2270
    },
    {
      "epoch": 0.46530612244897956,
      "grad_norm": 3.3834831714630127,
      "learning_rate": 0.00012,
      "loss": 0.8355,
      "step": 2280
    },
    {
      "epoch": 0.4673469387755102,
      "grad_norm": 6.959566593170166,
      "learning_rate": 0.00011833333333333334,
      "loss": 0.699,
      "step": 2290
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 5.102501392364502,
      "learning_rate": 0.00011666666666666667,
      "loss": 0.7924,
      "step": 2300
    },
    {
      "epoch": 0.4714285714285714,
      "grad_norm": 2.7949233055114746,
      "learning_rate": 0.000115,
      "loss": 0.6492,
      "step": 2310
    },
    {
      "epoch": 0.47346938775510206,
      "grad_norm": 5.758968353271484,
      "learning_rate": 0.00011333333333333333,
      "loss": 0.8453,
      "step": 2320
    },
    {
      "epoch": 0.47551020408163264,
      "grad_norm": 8.6907958984375,
      "learning_rate": 0.00011166666666666667,
      "loss": 0.6975,
      "step": 2330
    },
    {
      "epoch": 0.4775510204081633,
      "grad_norm": 4.368868350982666,
      "learning_rate": 0.00011,
      "loss": 0.751,
      "step": 2340
    },
    {
      "epoch": 0.47959183673469385,
      "grad_norm": 8.666573524475098,
      "learning_rate": 0.00010833333333333334,
      "loss": 0.757,
      "step": 2350
    },
    {
      "epoch": 0.4816326530612245,
      "grad_norm": 4.596561908721924,
      "learning_rate": 0.00010666666666666668,
      "loss": 0.8879,
      "step": 2360
    },
    {
      "epoch": 0.48367346938775513,
      "grad_norm": 4.396671772003174,
      "learning_rate": 0.000105,
      "loss": 0.6814,
      "step": 2370
    },
    {
      "epoch": 0.4857142857142857,
      "grad_norm": 4.518517017364502,
      "learning_rate": 0.00010333333333333333,
      "loss": 1.1199,
      "step": 2380
    },
    {
      "epoch": 0.48775510204081635,
      "grad_norm": 5.9089484214782715,
      "learning_rate": 0.00010166666666666667,
      "loss": 0.7834,
      "step": 2390
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 3.6449813842773438,
      "learning_rate": 0.0001,
      "loss": 0.7346,
      "step": 2400
    },
    {
      "epoch": 0.49183673469387756,
      "grad_norm": 4.175919055938721,
      "learning_rate": 9.833333333333333e-05,
      "loss": 0.7368,
      "step": 2410
    },
    {
      "epoch": 0.49387755102040815,
      "grad_norm": 3.4537789821624756,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.7043,
      "step": 2420
    },
    {
      "epoch": 0.4959183673469388,
      "grad_norm": 8.020825386047363,
      "learning_rate": 9.5e-05,
      "loss": 0.7137,
      "step": 2430
    },
    {
      "epoch": 0.49795918367346936,
      "grad_norm": 2.197882890701294,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.6391,
      "step": 2440
    },
    {
      "epoch": 0.5,
      "grad_norm": 3.275007963180542,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.7904,
      "step": 2450
    },
    {
      "epoch": 0.5020408163265306,
      "grad_norm": 5.9993791580200195,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.9549,
      "step": 2460
    },
    {
      "epoch": 0.5040816326530613,
      "grad_norm": 5.245082378387451,
      "learning_rate": 8.833333333333333e-05,
      "loss": 0.7801,
      "step": 2470
    },
    {
      "epoch": 0.5061224489795918,
      "grad_norm": 5.127850532531738,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.6955,
      "step": 2480
    },
    {
      "epoch": 0.5081632653061224,
      "grad_norm": 2.204957962036133,
      "learning_rate": 8.5e-05,
      "loss": 0.7059,
      "step": 2490
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 4.973205089569092,
      "learning_rate": 8.333333333333333e-05,
      "loss": 0.6734,
      "step": 2500
    },
    {
      "epoch": 0.5102040816326531,
      "eval_bleu-4": 0.0429461672578298,
      "eval_rouge-1": 29.103912095238094,
      "eval_rouge-2": 26.965298380952383,
      "eval_rouge-l": 7.718508380952382,
      "eval_runtime": 393.76,
      "eval_samples_per_second": 2.667,
      "eval_steps_per_second": 0.668,
      "step": 2500
    },
    {
      "epoch": 0.5122448979591837,
      "grad_norm": 1.4209787845611572,
      "learning_rate": 8.166666666666667e-05,
      "loss": 0.7886,
      "step": 2510
    },
    {
      "epoch": 0.5142857142857142,
      "grad_norm": 4.771307945251465,
      "learning_rate": 8e-05,
      "loss": 0.5994,
      "step": 2520
    },
    {
      "epoch": 0.5163265306122449,
      "grad_norm": 8.243791580200195,
      "learning_rate": 7.833333333333334e-05,
      "loss": 0.9596,
      "step": 2530
    },
    {
      "epoch": 0.5183673469387755,
      "grad_norm": 4.410280227661133,
      "learning_rate": 7.666666666666667e-05,
      "loss": 0.7346,
      "step": 2540
    },
    {
      "epoch": 0.5204081632653061,
      "grad_norm": 4.818938255310059,
      "learning_rate": 7.5e-05,
      "loss": 0.6299,
      "step": 2550
    },
    {
      "epoch": 0.5224489795918368,
      "grad_norm": 3.894002914428711,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.8123,
      "step": 2560
    },
    {
      "epoch": 0.5244897959183673,
      "grad_norm": 5.199617862701416,
      "learning_rate": 7.166666666666667e-05,
      "loss": 0.6273,
      "step": 2570
    },
    {
      "epoch": 0.5265306122448979,
      "grad_norm": 5.694707870483398,
      "learning_rate": 7.000000000000001e-05,
      "loss": 0.7725,
      "step": 2580
    },
    {
      "epoch": 0.5285714285714286,
      "grad_norm": 2.518068552017212,
      "learning_rate": 6.833333333333333e-05,
      "loss": 0.6428,
      "step": 2590
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 2.72359299659729,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.6016,
      "step": 2600
    },
    {
      "epoch": 0.5326530612244897,
      "grad_norm": 6.603453636169434,
      "learning_rate": 6.500000000000001e-05,
      "loss": 0.7262,
      "step": 2610
    },
    {
      "epoch": 0.5346938775510204,
      "grad_norm": 2.2352237701416016,
      "learning_rate": 6.333333333333335e-05,
      "loss": 0.7651,
      "step": 2620
    },
    {
      "epoch": 0.536734693877551,
      "grad_norm": 4.149991035461426,
      "learning_rate": 6.166666666666667e-05,
      "loss": 0.866,
      "step": 2630
    },
    {
      "epoch": 0.5387755102040817,
      "grad_norm": 2.087883234024048,
      "learning_rate": 6e-05,
      "loss": 0.6346,
      "step": 2640
    },
    {
      "epoch": 0.5408163265306123,
      "grad_norm": 2.3194406032562256,
      "learning_rate": 5.833333333333333e-05,
      "loss": 0.5394,
      "step": 2650
    },
    {
      "epoch": 0.5428571428571428,
      "grad_norm": 7.3235015869140625,
      "learning_rate": 5.6666666666666664e-05,
      "loss": 0.843,
      "step": 2660
    },
    {
      "epoch": 0.5448979591836735,
      "grad_norm": 3.7343556880950928,
      "learning_rate": 5.5e-05,
      "loss": 0.7096,
      "step": 2670
    },
    {
      "epoch": 0.5469387755102041,
      "grad_norm": 6.87615442276001,
      "learning_rate": 5.333333333333334e-05,
      "loss": 0.9089,
      "step": 2680
    },
    {
      "epoch": 0.5489795918367347,
      "grad_norm": 2.1805877685546875,
      "learning_rate": 5.1666666666666664e-05,
      "loss": 0.4658,
      "step": 2690
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 1.3129037618637085,
      "learning_rate": 5e-05,
      "loss": 0.7506,
      "step": 2700
    },
    {
      "epoch": 0.5530612244897959,
      "grad_norm": 5.099262237548828,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.7117,
      "step": 2710
    },
    {
      "epoch": 0.5551020408163265,
      "grad_norm": 2.8819780349731445,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.6643,
      "step": 2720
    },
    {
      "epoch": 0.5571428571428572,
      "grad_norm": 7.7265472412109375,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 0.9033,
      "step": 2730
    },
    {
      "epoch": 0.5591836734693878,
      "grad_norm": 3.7787163257598877,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.9916,
      "step": 2740
    },
    {
      "epoch": 0.5612244897959183,
      "grad_norm": 4.723351001739502,
      "learning_rate": 4.1666666666666665e-05,
      "loss": 0.7178,
      "step": 2750
    },
    {
      "epoch": 0.563265306122449,
      "grad_norm": 8.095650672912598,
      "learning_rate": 4e-05,
      "loss": 0.8104,
      "step": 2760
    },
    {
      "epoch": 0.5653061224489796,
      "grad_norm": 3.7639753818511963,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.6686,
      "step": 2770
    },
    {
      "epoch": 0.5673469387755102,
      "grad_norm": 4.740838050842285,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.5228,
      "step": 2780
    },
    {
      "epoch": 0.5693877551020409,
      "grad_norm": 3.4338040351867676,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 0.6799,
      "step": 2790
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 2.5496914386749268,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.5697,
      "step": 2800
    },
    {
      "epoch": 0.573469387755102,
      "grad_norm": 5.053775310516357,
      "learning_rate": 3.166666666666667e-05,
      "loss": 0.7939,
      "step": 2810
    },
    {
      "epoch": 0.5755102040816327,
      "grad_norm": 7.554577827453613,
      "learning_rate": 3e-05,
      "loss": 0.8865,
      "step": 2820
    },
    {
      "epoch": 0.5775510204081633,
      "grad_norm": 5.223698139190674,
      "learning_rate": 2.8333333333333332e-05,
      "loss": 0.6,
      "step": 2830
    },
    {
      "epoch": 0.5795918367346938,
      "grad_norm": 4.862609386444092,
      "learning_rate": 2.666666666666667e-05,
      "loss": 0.5596,
      "step": 2840
    },
    {
      "epoch": 0.5816326530612245,
      "grad_norm": 2.9381587505340576,
      "learning_rate": 2.5e-05,
      "loss": 0.6837,
      "step": 2850
    },
    {
      "epoch": 0.5836734693877551,
      "grad_norm": 3.437474012374878,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.5305,
      "step": 2860
    },
    {
      "epoch": 0.5857142857142857,
      "grad_norm": 4.425753593444824,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.7218,
      "step": 2870
    },
    {
      "epoch": 0.5877551020408164,
      "grad_norm": 9.913938522338867,
      "learning_rate": 2e-05,
      "loss": 0.9836,
      "step": 2880
    },
    {
      "epoch": 0.5897959183673469,
      "grad_norm": 4.217766761779785,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.6734,
      "step": 2890
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 4.1726603507995605,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.6538,
      "step": 2900
    },
    {
      "epoch": 0.5938775510204082,
      "grad_norm": 5.612740993499756,
      "learning_rate": 1.5e-05,
      "loss": 0.8708,
      "step": 2910
    },
    {
      "epoch": 0.5959183673469388,
      "grad_norm": 4.701681613922119,
      "learning_rate": 1.3333333333333335e-05,
      "loss": 0.822,
      "step": 2920
    },
    {
      "epoch": 0.5979591836734693,
      "grad_norm": 4.990236759185791,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.6538,
      "step": 2930
    },
    {
      "epoch": 0.6,
      "grad_norm": 2.193694829940796,
      "learning_rate": 1e-05,
      "loss": 0.6946,
      "step": 2940
    },
    {
      "epoch": 0.6020408163265306,
      "grad_norm": 3.5432775020599365,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.6799,
      "step": 2950
    },
    {
      "epoch": 0.6040816326530613,
      "grad_norm": 3.177426338195801,
      "learning_rate": 6.6666666666666675e-06,
      "loss": 0.5957,
      "step": 2960
    },
    {
      "epoch": 0.6061224489795919,
      "grad_norm": 3.212033748626709,
      "learning_rate": 5e-06,
      "loss": 0.8646,
      "step": 2970
    },
    {
      "epoch": 0.6081632653061224,
      "grad_norm": 6.149057388305664,
      "learning_rate": 3.3333333333333337e-06,
      "loss": 0.605,
      "step": 2980
    },
    {
      "epoch": 0.610204081632653,
      "grad_norm": 3.369886875152588,
      "learning_rate": 1.6666666666666669e-06,
      "loss": 0.6969,
      "step": 2990
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 4.540225028991699,
      "learning_rate": 0.0,
      "loss": 0.7017,
      "step": 3000
    },
    {
      "epoch": 0.6122448979591837,
      "eval_bleu-4": 0.042378563706074616,
      "eval_rouge-1": 28.421990285714287,
      "eval_rouge-2": 26.309561809523814,
      "eval_rouge-l": 7.617832761904762,
      "eval_runtime": 575.9753,
      "eval_samples_per_second": 1.823,
      "eval_steps_per_second": 0.457,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0618256464478208e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
